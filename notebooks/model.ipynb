{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to pytorch lightning fitting routine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bisect\n",
    "import tensorboard\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# from kws.eval import Metric\n",
    "\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = 1e-3\n",
    "        self.validation_step_outputs = []\n",
    "        self.training_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_hat = self.model(x)\n",
    "        return y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x = batch[\"x\"]\n",
    "        y = batch[\"y\"]\n",
    "        y_hat = self(x)\n",
    "        y_hat = y_hat.squeeze()\n",
    "\n",
    "        # dummy metrics\n",
    "        metrics_dict = {\"loss\": 10, \"train_EM\": 0.9, \"train_F1\": 0.9}\n",
    "\n",
    "        # loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "\n",
    "        # y_hat = (F.sigmoid(y_hat) > 0.5).float()\n",
    "\n",
    "        # metrics = self.metric(y_hat, y)()\n",
    "        # metrics_dict = {\n",
    "        #     \"loss\": loss,\n",
    "        #     \"train_ttr\": metrics.ttr,\n",
    "        #     \"train_ftr\": metrics.ftr,\n",
    "        #     \"train_acc\": metrics.acc,\n",
    "        # }\n",
    "        self.training_step_outputs.append(metrics_dict)\n",
    "        return metrics_dict\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        results = {\n",
    "            \"loss\": torch.tensor(\n",
    "                [x[\"loss\"] for x in self.training_step_outputs]\n",
    "            ).mean(),\n",
    "            \"F1\": torch.tensor(\n",
    "                [x[\"train_F1\"] for x in self.training_step_outputs]\n",
    "            ).mean(),\n",
    "            \"EM\": torch.tensor(\n",
    "                [x[\"train_EM\"] for x in self.training_step_outputs]\n",
    "            ).mean(),\n",
    "        }\n",
    "        # self.log(f\"LR\",self.lr, on_epoch=True, prog_bar=True, logger=True)\n",
    "        for k, v in results.items():\n",
    "            self.log(\n",
    "                f\"train_{k}\",\n",
    "                v,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "                sync_dist=True,\n",
    "            )\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch[\"x\"]\n",
    "        y = batch[\"y\"]\n",
    "        y_hat = self(x)\n",
    "        # (batch, num_classes)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        # (batch,)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        # pred = F.sigmoid(y_hat)\n",
    "        # y_hat = (pred > 0.5).float()\n",
    "\n",
    "        # metrics = self.metric(y_hat, y)()\n",
    "        # metrics_dict = {\n",
    "        #     \"val_loss\": loss,\n",
    "        #     \"val_ttr\": metrics.ttr,\n",
    "        #     \"val_ftr\": metrics.ftr,\n",
    "        #     \"val_acc\": metrics.acc,\n",
    "        # }\n",
    "\n",
    "        # dummy metrics\n",
    "        metrics_dict = {\"loss\": 10, \"val_EM\": 0.9, \"val_F1\": 0.9}\n",
    "        self.validation_step_outputs.append(metrics_dict)\n",
    "        return metrics_dict\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        results = {\n",
    "            \"loss\": torch.tensor(\n",
    "                [x[\"val_loss\"] for x in self.validation_step_outputs]\n",
    "            ).mean(),\n",
    "            \"EM\": torch.tensor(\n",
    "                [x[\"val_EM\"] for x in self.validation_step_outputs]\n",
    "            ).mean(),\n",
    "            \"F1\": torch.tensor(\n",
    "                [x[\"val_F1\"] for x in self.validation_step_outputs]\n",
    "            ).mean(),\n",
    "        }\n",
    "        for k, v in results.items():\n",
    "            self.log(\n",
    "                f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True, sync_dist=True\n",
    "            )\n",
    "            # self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True) # , logger=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch[\"x\"]\n",
    "        y = batch[\"y\"]\n",
    "        y_hat = self(x)\n",
    "        # (batch, num_classes)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        # (batch,)\n",
    "        pred = F.sigmoid(y_hat)\n",
    "\n",
    "        # (batch_probabilities,)\n",
    "        # y_hat = (pred > 0.5).float()\n",
    "        # (batch_labels,)\n",
    "        # metrics = self.metric(y_hat, y)()\n",
    "\n",
    "        metrics_dict = {\n",
    "            \"test_EM\": 0.9,\n",
    "            \"test_F1\": 0.8,\n",
    "        }\n",
    "        self.test_step_outputs.append(metrics_dict)\n",
    "        return metrics_dict\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        results = {\n",
    "            \"F1\": torch.tensor([x[\"test_EM\"] for x in self.test_step_outputs]).mean(),\n",
    "            \"EM\": torch.tensor([x[\"test_F1\"] for x in self.test_step_outputs]).mean(),\n",
    "        }\n",
    "\n",
    "        for k, v in results.items():\n",
    "            self.log(\n",
    "                f\"test_{k}\",\n",
    "                v,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "                sync_dist=True,\n",
    "            )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        # special scheduler for transformers\n",
    "        optimizer = optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=self.cfg_fitting.learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-08,\n",
    "            weight_decay=0.05,\n",
    "        )\n",
    "\n",
    "        def lr_scheduler_lambda_1(epoch):\n",
    "            if epoch < 3:\n",
    "                # warm up lr\n",
    "                lr_scale = self.cfg_fitting.lr_rate[epoch]\n",
    "            else:\n",
    "                # warmup schedule\n",
    "                lr_pos = int(\n",
    "                    -1 - bisect.bisect_left(self.cfg_fitting.lr_scheduler_epoch, epoch)\n",
    "                )\n",
    "                if lr_pos < -3:\n",
    "                    lr_scale = max(self.cfg_fitting.lr_rate[0] * (0.98**epoch), 0.03)\n",
    "                else:\n",
    "                    lr_scale = self.cfg_fitting.lr_rate[lr_pos]\n",
    "            return lr_scale\n",
    "\n",
    "        scheduler_1 = optim.lr_scheduler.LambdaLR(\n",
    "            optimizer, lr_lambda=lr_scheduler_lambda_1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler_1,\n",
    "            \"monitor\": \"val_loss\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from nlp import Dataset\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from babl.data import convert_to_features\n",
    "import random\n",
    "import json\n",
    "from babl.data import T2TDataCollator\n",
    "from dataclasses import dataclass\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    ")\n",
    "\n",
    "\n",
    "class CallbackCollection:\n",
    "    def __init__(self, model_name, data_path) -> None:\n",
    "\n",
    "        ####################################################################################\n",
    "        @dataclass\n",
    "        class FittingArgs:\n",
    "            es_patience: int = 5\n",
    "            model_dir: str = str(Path(\"/home/ola/Code/babl/outputs\") / model_name)\n",
    "\n",
    "        ####################################################################################\n",
    "\n",
    "        self.data_path = data_path\n",
    "\n",
    "        self.args = FittingArgs()\n",
    "\n",
    "    def __call__(self):\n",
    "        lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            mode=\"min\", monitor=\"val_loss\", patience=self.args.es_patience\n",
    "        )\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            monitor=\"val_loss\",\n",
    "            dirpath=self.args.model_dir,\n",
    "            save_top_k=2,\n",
    "            save_last=True,\n",
    "            mode=\"min\",\n",
    "            filename=\"{epoch}-{val_loss:.2f}-{val_acc:.2f}-{val_ttr:.2f}-{val_ftr:.2f}\",\n",
    "        )\n",
    "\n",
    "        callbacks = {\n",
    "            \"checkpoint\": checkpoint_callback,\n",
    "            \"lr\": lr_monitor,\n",
    "            \"es\": early_stopping,\n",
    "        }\n",
    "        # callbacks = [checkpoint_callback, lr_monitor, early_stopping]\n",
    "        return callbacks\n",
    "\n",
    "\n",
    "# class TextDataset(Dataset):\n",
    "#     def __init__(self, dath_path, tokenizer):\n",
    "#         # super().__init__()\n",
    "#         self.data_path = Path(dath_path)\n",
    "#         self.tokenizer = tokenizer\n",
    "\n",
    "#     def read(\n",
    "#         self,\n",
    "#     ):\n",
    "#         examples = []\n",
    "#         with open(self.data_path, \"r\") as json_file:\n",
    "#             x = list(json_file)\n",
    "#             # logger.debug(f\"[data.py::build_dataset]{x=}\")\n",
    "#             for json_str in x:\n",
    "#                 examples.append(json.loads(json_str))\n",
    "#         return examples\n",
    "\n",
    "#     def extract_valid_pairs(self, samples):\n",
    "#         valid_questions = []\n",
    "#         for l in samples:\n",
    "#             # clear all docs with more or less than one answer\n",
    "#             # clean all docs with no annotations\n",
    "#             if len(l[\"annotations\"][0][\"short_answers\"]) == 1:\n",
    "#                 if len(l[\"long_answer_candidates\"]) > 2:\n",
    "#                     valid_questions.append(l)\n",
    "#         return valid_questions\n",
    "\n",
    "#     def encode(self, dataset):\n",
    "#         ####################################################################################\n",
    "#         @dataclass\n",
    "#         class Args:\n",
    "#             input_max_len: int = 64\n",
    "#             output_max_len: int = 64\n",
    "\n",
    "#         ####################################################################################\n",
    "\n",
    "#         txt2feats = partial(convert_to_features, args=Args(), tokenizer=self.tokenizer)\n",
    "#         # map convert_to_features batch wise\n",
    "#         ds = dataset.map(txt2feats, batched=True)\n",
    "#         # set the tensor type and the columns which the dataset should return\n",
    "#         columns = [\"input_ids\", \"target_ids\", \"attention_mask\", \"target_attention_mask\"]\n",
    "#         ds.set_format(type=\"torch\", columns=columns)\n",
    "#         return ds\n",
    "\n",
    "#     def construct_ds(self, examples):\n",
    "#         datapoints = {}\n",
    "#         datapoints[\"input_text\"] = []\n",
    "#         datapoints[\"target_text\"] = []\n",
    "#         for i, q in enumerate(examples):\n",
    "\n",
    "#             # fitting dataset; positive and negative fitting examples\n",
    "#             if random.randint(0, 1) == 1:\n",
    "#                 # Construct positive example\n",
    "#                 datapoints[\"input_text\"].append(\n",
    "#                     f\"question: {q['question_text']}  context: {self.get_long_answer(q)} </s>\"\n",
    "#                 )\n",
    "#                 datapoints[\"target_text\"].append(self.get_short_answer(q))\n",
    "#                 # if i % 10000 == 0:\n",
    "#                 #     print(\"-\"*100)\n",
    "#                 #     print(\"Positive fitting example:\")\n",
    "#                 #     print(f\"[input_text]: question: {q['question_text']}  context: {get_long_answer(q)} </s>\")\n",
    "#                 #     print(f\"[target_text]: {get_short_answer(q)}\")\n",
    "#                 #     print(\"-\"*100)\n",
    "#             else:\n",
    "#                 # Construct negative example\n",
    "#                 datapoints[\"input_text\"].append(\n",
    "#                     f\"question: {q['question_text']}  context: {self.get_random_negative(q)} </s>\"\n",
    "#                 )\n",
    "#                 datapoints[\"target_text\"].append(\"None </s>\")\n",
    "#                 # if i % 10000 == 0:\n",
    "#                 #     print(\"-\"*100)\n",
    "#                 #     print(\"negative fitting example:\")\n",
    "#                 #     print(f\"[input_text]: question: {q['question_text']}  context: {get_random_negative(q)} </s>\")\n",
    "#                 #     print(f\"[target_text]: None </s>\")\n",
    "#                 #     print(\"-\"*100)\n",
    "#         assert len(datapoints[\"target_text\"]) == len(\n",
    "#             datapoints[\"input_text\"]\n",
    "#         ), \"incorrect data distribution\"\n",
    "\n",
    "#         return self.encode(super().from_dict(datapoints))\n",
    "\n",
    "#     def get_exert(self, doc, start_token, end_token):\n",
    "#         return \" \".join(doc.split(\" \")[start_token:end_token])\n",
    "\n",
    "#     def get_short_answer(self, q):\n",
    "#         answer_indx = q[\"annotations\"][0][\"short_answers\"][0]\n",
    "#         return self.get_exert(\n",
    "#             q[\"document_text\"], answer_indx[\"start_token\"], answer_indx[\"end_token\"]\n",
    "#         )\n",
    "\n",
    "#     def get_long_answer(self, q):\n",
    "#         answer_indx = q[\"annotations\"][0][\"long_answer\"]\n",
    "#         return self.get_exert(\n",
    "#             q[\"document_text\"], answer_indx[\"start_token\"], answer_indx[\"end_token\"]\n",
    "#         )\n",
    "\n",
    "#     def get_random_negative(self, q):\n",
    "#         long_answer_indx = q[\"annotations\"][0][\"long_answer\"]\n",
    "\n",
    "#         for i in range(len(q[\"long_answer_candidates\"])):\n",
    "#             if (\n",
    "#                 q[\"long_answer_candidates\"][i][\"start_token\"]\n",
    "#                 == long_answer_indx[\"start_token\"]\n",
    "#             ):\n",
    "#                 del q[\"long_answer_candidates\"][i]\n",
    "#                 break\n",
    "\n",
    "#         answer_indx = random.choice(q[\"long_answer_candidates\"])\n",
    "#         return self.get_exert(\n",
    "#             q[\"document_text\"], answer_indx[\"start_token\"], answer_indx[\"end_token\"]\n",
    "#         )\n",
    "\n",
    "\n",
    "class TextDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path, tokenizer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_path = Path(data_path) / \"50k.jsonl\"\n",
    "        self.val_path = Path(data_path) / \"10k.jsonl\"\n",
    "        # NOTICE, we  re-use the validatio dataset\n",
    "        self.test_path = Path(data_path) / \"10k.jsonl\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pin_memory = False  # True if torch.cuda.is_available() else False\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        ds_train = TextDataset(self.train_path, self.tokenizer)\n",
    "        return DataLoader(\n",
    "            ds_train,\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=T2TDataCollator(),\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        ds_val = TextDataset(self.val_path, self.tokenizer)\n",
    "        return DataLoader(\n",
    "            ds_val,\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=T2TDataCollator(),\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "\n",
    "        ds_test = TextDataset(self.test_path, self.tokenizer)\n",
    "        return DataLoader(\n",
    "            ds_test,\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=T2TDataCollator(),\n",
    "        )\n",
    "\n",
    "\n",
    "# class Predictor(nn.Module):\n",
    "#     def __init__(self, model):\n",
    "#         super().__init__()\n",
    "#         self.model = model\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         logits = self.model(x)\n",
    "#         pred = F.sigmoid(logits)\n",
    "#         return pred\n",
    "\n",
    "\n",
    "class Fitter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        model_name,\n",
    "        data_path=\"/home/ola/Code/babl/inputs\",\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_name = model_name\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def setup(self):\n",
    "        data_module = TextDataModule(data_path=self.data_path, tokenizer=self.tokenizer)\n",
    "\n",
    "        train_loader = data_module.train_dataloader()\n",
    "        val_loader = data_module.val_dataloader()\n",
    "        test_loader = data_module.test_dataloader()\n",
    "\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    def callbacks(self):\n",
    "        # cfg_fitting = self.cfg_fitting\n",
    "        callback_collection = CallbackCollection(self.model_name, self.data_path)\n",
    "        return callback_collection()\n",
    "\n",
    "    def __call__(self):\n",
    "\n",
    "        ####################################################################################\n",
    "        @dataclass\n",
    "        class FittingArgs:\n",
    "            es_patience: int = 5\n",
    "            model_dir: str = str(Path(\"/home/ola/Code/babl/outputs\") / self.model_name)\n",
    "\n",
    "        ####################################################################################\n",
    "\n",
    "        args = FittingArgs()\n",
    "\n",
    "        logger = TensorBoardLogger(\n",
    "            save_dir=args.model_dir,\n",
    "            name=\"lightning_logs\",\n",
    "        )\n",
    "        Model = self.model\n",
    "        # get loaders and datamodule to access input shape\n",
    "        train_loader, val_loader, test_loader = self.setup()\n",
    "        print(\"Created training, validating and test loaders .... \")\n",
    "        # get input shape for onnx exporting\n",
    "        # input_shape = data_module.input_shape\n",
    "        # init model\n",
    "        # kwargs = {}\n",
    "        # model = Model(**kwargs)\n",
    "\n",
    "        # setup training, validating and testing routines for the model\n",
    "        routine = Routine(self.model)\n",
    "\n",
    "        # Init a trainer to execute routine\n",
    "        callback_dict = self.callbacks()\n",
    "        callback_list = [v for (_, v) in callback_dict.items()]\n",
    "        number_devices = os.getenv(\"CUDA_VISIBLE_DEVICES\", \"1,\").split(\",\")\n",
    "        try:\n",
    "            number_devices.remove(\"\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        ####################################################################################\n",
    "        @dataclass\n",
    "        class FittingArgs:\n",
    "            max_epoch: int = 10\n",
    "            fast_dev_run: bool = True\n",
    "\n",
    "        ####################################################################################\n",
    "\n",
    "        args = FittingArgs()\n",
    "        trainer = Trainer(\n",
    "            accelerator=\"gpu\",\n",
    "            devices=len(number_devices),\n",
    "            strategy=os.getenv(\"STRATEGY\", \"ddp\"),\n",
    "            sync_batchnorm=True,\n",
    "            logger=logger,\n",
    "            max_epochs=args.max_epoch,\n",
    "            callbacks=callback_list,\n",
    "            num_sanity_val_steps=2,\n",
    "            # resume_from_checkpoint=self.cfg_fitting.resume_from_checkpoint,\n",
    "            gradient_clip_val=1.0,\n",
    "            fast_dev_run=args.fast_dev_run,\n",
    "        )\n",
    "\n",
    "        trainer.fit(\n",
    "            routine, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    "        )  # ,ckpt_path=PATH)\n",
    "\n",
    "        if args.fast_dev_run:\n",
    "            # issue with finding best weights path for in fast dev run using last model weights\n",
    "            model_ckpt_path = callback_dict[\"checkpoint\"].__dict__[\"last_model_path\"]\n",
    "        else:\n",
    "            model_ckpt_path = callback_dict[\"checkpoint\"].__dict__[\"best_model_path\"]\n",
    "\n",
    "        trainer.test(\n",
    "            dataloaders=test_loader,\n",
    "            ckpt_path=model_ckpt_path,\n",
    "        )\n",
    "        # Return the input_shapes and trainer of the model for exporting\n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import random\n",
    "import json\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "from babl.data import convert_to_features\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dath_path, tokenizer, plain_text=False):\n",
    "        # super().__init__()\n",
    "        self.data_path = Path(dath_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.plain_text = plain_text\n",
    "        self.ds = {}\n",
    "        if plain_text:\n",
    "            self.construct_ds(self.extract_valid_pairs(self.read()))\n",
    "        else:\n",
    "\n",
    "            self.construct_ds(self.extract_valid_pairs(self.read()))\n",
    "            self.ds = self.convert_to_features(self.ds)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds.values()[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.plain_text:\n",
    "            return {\n",
    "                \"input_text\": self.ds[\"input_text\"][idx],\n",
    "                \"target_text\": self.ds[\"target_text\"][idx],\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"input_ids\": self.ds[\"input_ids\"][idx],\n",
    "                \"attention_mask\": self.ds[\"attention_mask\"][idx],\n",
    "                \"target_ids\": self.ds[\"input_ids\"][idx],\n",
    "                \"target_attention_mask\": self.ds[\"attention_mask\"][idx],\n",
    "            }\n",
    "\n",
    "    def read(\n",
    "        self,\n",
    "    ):\n",
    "        examples = []\n",
    "        with open(self.data_path, \"r\") as json_file:\n",
    "            x = list(json_file)\n",
    "            # logger.debug(f\"[data.py::build_dataset]{x=}\")\n",
    "            for json_str in x:\n",
    "                examples.append(json.loads(json_str))\n",
    "        return examples\n",
    "\n",
    "    def extract_valid_pairs(self, samples):\n",
    "        valid_questions = []\n",
    "        for l in samples:\n",
    "            # clear all docs with more or less than one answer\n",
    "            # clean all docs with no annotations\n",
    "            if len(l[\"annotations\"][0][\"short_answers\"]) == 1:\n",
    "                if len(l[\"long_answer_candidates\"]) > 2:\n",
    "                    valid_questions.append(l)\n",
    "        return valid_questions\n",
    "\n",
    "    def construct_ds(self, examples):\n",
    "\n",
    "        self.ds[\"input_text\"] = []\n",
    "        self.ds[\"target_text\"] = []\n",
    "        for i, q in enumerate(examples):\n",
    "\n",
    "            # fitting dataset; positive and negative fitting examples\n",
    "            if random.randint(0, 1) == 1:\n",
    "                # Construct positive example\n",
    "                self.ds[\"input_text\"].append(\n",
    "                    f\"question: {q['question_text']}  context: {self.get_long_answer(q)} </s>\"\n",
    "                )\n",
    "                self.ds[\"target_text\"].append(self.get_short_answer(q))\n",
    "                # if i % 10000 == 0:\n",
    "                #     print(\"-\"*100)\n",
    "                #     print(\"Positive fitting example:\")\n",
    "                #     print(f\"[input_text]: question: {q['question_text']}  context: {get_long_answer(q)} </s>\")\n",
    "                #     print(f\"[target_text]: {get_short_answer(q)}\")\n",
    "                #     print(\"-\"*100)\n",
    "            else:\n",
    "                # Construct negative example\n",
    "                self.ds[\"input_text\"].append(\n",
    "                    f\"question: {q['question_text']}  context: {self.get_random_negative(q)} </s>\"\n",
    "                )\n",
    "                self.ds[\"target_text\"].append(\"None </s>\")\n",
    "                # if i % 10000 == 0:\n",
    "                #     print(\"-\"*100)\n",
    "                #     print(\"negative fitting example:\")\n",
    "                #     print(f\"[input_text]: question: {q['question_text']}  context: {get_random_negative(q)} </s>\")\n",
    "                #     print(f\"[target_text]: None </s>\")\n",
    "                #     print(\"-\"*100)\n",
    "        assert len(self.ds[\"target_text\"]) == len(\n",
    "            self.ds[\"input_text\"]\n",
    "        ), \"incorrect data distribution\"\n",
    "\n",
    "    def convert_to_features(self, batch):\n",
    "\n",
    "        # tokenizer = T5Tokenizer.from_pretrained(args.model_name_or_path)  # \"t5-small\")\n",
    "        ####################################################################################\n",
    "        @dataclass\n",
    "        class Args:\n",
    "            input_max_len: int = 64\n",
    "            output_max_len: int = 64\n",
    "\n",
    "            ####################################################################################\n",
    "\n",
    "        input_encodings = self.tokenizer.batch_encode_plus(\n",
    "            batch[\"input_text\"],\n",
    "            truncation=True,\n",
    "            pad_to_max_length=True,\n",
    "            max_length=Args().input_max_len,\n",
    "        )\n",
    "        target_encodings = self.tokenizer.batch_encode_plus(\n",
    "            batch[\"target_text\"],\n",
    "            truncation=True,\n",
    "            pad_to_max_length=True,\n",
    "            max_length=Args().output_max_len,\n",
    "        )\n",
    "        # print(\"input_encodings\", input_encodings.keys())\n",
    "        # print(\"target_encodings\", target_encodings.keys())\n",
    "        encodings = {\n",
    "            \"input_ids\": input_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "            \"target_ids\": target_encodings[\"input_ids\"],\n",
    "            \"target_attention_mask\": target_encodings[\"attention_mask\"],\n",
    "        }\n",
    "        return encodings\n",
    "\n",
    "    # def encode(self, dataset):\n",
    "    #     ####################################################################################\n",
    "    #     @dataclass\n",
    "    #     class Args:\n",
    "    #         input_max_len: int = 64\n",
    "    #         output_max_len: int = 64\n",
    "\n",
    "    #     ####################################################################################\n",
    "    #     txt2feats = partial(convert_to_features, args=Args(), tokenizer=self.tokenizer)\n",
    "    #     ds = self.construct_ds(self.extract_valid_pairs(self.read()))\n",
    "    #     return ds\n",
    "\n",
    "    def get_exert(self, doc, start_token, end_token):\n",
    "        return \" \".join(doc.split(\" \")[start_token:end_token])\n",
    "\n",
    "    def get_short_answer(self, q):\n",
    "        answer_indx = q[\"annotations\"][0][\"short_answers\"][0]\n",
    "        return self.get_exert(\n",
    "            q[\"document_text\"], answer_indx[\"start_token\"], answer_indx[\"end_token\"]\n",
    "        )\n",
    "\n",
    "    def get_long_answer(self, q):\n",
    "        answer_indx = q[\"annotations\"][0][\"long_answer\"]\n",
    "        return self.get_exert(\n",
    "            q[\"document_text\"], answer_indx[\"start_token\"], answer_indx[\"end_token\"]\n",
    "        )\n",
    "\n",
    "    def get_random_negative(self, q):\n",
    "        long_answer_indx = q[\"annotations\"][0][\"long_answer\"]\n",
    "\n",
    "        for i in range(len(q[\"long_answer_candidates\"])):\n",
    "            if (\n",
    "                q[\"long_answer_candidates\"][i][\"start_token\"]\n",
    "                == long_answer_indx[\"start_token\"]\n",
    "            ):\n",
    "                del q[\"long_answer_candidates\"][i]\n",
    "                break\n",
    "\n",
    "        answer_indx = random.choice(q[\"long_answer_candidates\"])\n",
    "        return self.get_exert(\n",
    "            q[\"document_text\"], answer_indx[\"start_token\"], answer_indx[\"end_token\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babl.models import MODELS_CHOICES, MODELS\n",
    "from pathlib import Path\n",
    "\n",
    "model_name = \"t5\"\n",
    "full_model_name = MODELS_CHOICES[model_name][0]\n",
    "t_w_m = MODELS[model_name]\n",
    "\n",
    "tokenizer = t_w_m[\"tok\"]\n",
    "model = t_w_m[\"model\"]\n",
    "\n",
    "t = tokenizer.from_pretrained(full_model_name)\n",
    "m = model.from_pretrained(full_model_name)\n",
    "\n",
    "data_path = Path(\"/home/ola/Code/babl/inputs\") / \"10k.jsonl\"\n",
    "\n",
    "ds = TextDataset(data_path, tokenizer=t, plain_text=True)\n",
    "\n",
    "# data_module = TextDataModule(data_path, tokenizer)\n",
    "\n",
    "# Fitter(model=m, model_name=full_model_name, tokenizer=tokenizer, data_path=data_path)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('question: who played the devil in storm of the century  context: <Li> Richard Fitzpatrick as Jonas Stanhope </Li> </s>',\n",
       " 'None </s>')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3194"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.ds[\"input_text\"].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babl-f0iBC2ut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
