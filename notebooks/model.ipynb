{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to pytorch lightning fitting routine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bisect\n",
    "import tensorboard\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# from kws.eval import Metric\n",
    "\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = 1e-3\n",
    "        self.validation_step_outputs = []\n",
    "        self.training_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "    ):\n",
    "        y_hat = self.model(x)\n",
    "        return y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        print(f\"keys = {batch.keys()}\")\n",
    "        print(f\"{batch=}\")\n",
    "\n",
    "        y_hat = self(**batch)\n",
    "        print(f\"{y_hat=}\")\n",
    "\n",
    "        y_hat = y_hat.squeeze()\n",
    "\n",
    "        # dummy metrics\n",
    "        metrics_dict = {\"loss\": 10, \"train_EM\": 0.9, \"train_F1\": 0.9}\n",
    "\n",
    "        # loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "\n",
    "        # y_hat = (F.sigmoid(y_hat) > 0.5).float()\n",
    "\n",
    "        # metrics = self.metric(y_hat, y)()\n",
    "        # metrics_dict = {\n",
    "        #     \"loss\": loss,\n",
    "        #     \"train_ttr\": metrics.ttr,\n",
    "        #     \"train_ftr\": metrics.ftr,\n",
    "        #     \"train_acc\": metrics.acc,\n",
    "        # }\n",
    "        self.training_step_outputs.append(metrics_dict)\n",
    "        return metrics_dict\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        results = {\n",
    "            \"loss\": torch.tensor(\n",
    "                [x[\"loss\"] for x in self.training_step_outputs]\n",
    "            ).mean(),\n",
    "            \"F1\": torch.tensor(\n",
    "                [x[\"train_F1\"] for x in self.training_step_outputs]\n",
    "            ).mean(),\n",
    "            \"EM\": torch.tensor(\n",
    "                [x[\"train_EM\"] for x in self.training_step_outputs]\n",
    "            ).mean(),\n",
    "        }\n",
    "        # self.log(f\"LR\",self.lr, on_epoch=True, prog_bar=True, logger=True)\n",
    "        for k, v in results.items():\n",
    "            self.log(\n",
    "                f\"train_{k}\",\n",
    "                v,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "                sync_dist=True,\n",
    "            )\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # x = batch[\"x\"]\n",
    "        # y = batch[\"y\"]\n",
    "        y_hat = self(**batch)\n",
    "        # (batch, num_classes)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        # (batch,)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        # pred = F.sigmoid(y_hat)\n",
    "        # y_hat = (pred > 0.5).float()\n",
    "\n",
    "        # metrics = self.metric(y_hat, y)()\n",
    "        # metrics_dict = {\n",
    "        #     \"val_loss\": loss,\n",
    "        #     \"val_ttr\": metrics.ttr,\n",
    "        #     \"val_ftr\": metrics.ftr,\n",
    "        #     \"val_acc\": metrics.acc,\n",
    "        # }\n",
    "\n",
    "        # dummy metrics\n",
    "        metrics_dict = {\"loss\": 10, \"val_EM\": 0.9, \"val_F1\": 0.9}\n",
    "        self.validation_step_outputs.append(metrics_dict)\n",
    "        return metrics_dict\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        results = {\n",
    "            \"loss\": torch.tensor(\n",
    "                [x[\"val_loss\"] for x in self.validation_step_outputs]\n",
    "            ).mean(),\n",
    "            \"EM\": torch.tensor(\n",
    "                [x[\"val_EM\"] for x in self.validation_step_outputs]\n",
    "            ).mean(),\n",
    "            \"F1\": torch.tensor(\n",
    "                [x[\"val_F1\"] for x in self.validation_step_outputs]\n",
    "            ).mean(),\n",
    "        }\n",
    "        for k, v in results.items():\n",
    "            self.log(\n",
    "                f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True, sync_dist=True\n",
    "            )\n",
    "            # self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True) # , logger=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # x = batch[\"x\"]\n",
    "        # y = batch[\"y\"]\n",
    "        y_hat = self(**batch)\n",
    "        # (batch, num_classes)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        # (batch,)\n",
    "        pred = F.sigmoid(y_hat)\n",
    "\n",
    "        # (batch_probabilities,)\n",
    "        # y_hat = (pred > 0.5).float()\n",
    "        # (batch_labels,)\n",
    "        # metrics = self.metric(y_hat, y)()\n",
    "\n",
    "        metrics_dict = {\n",
    "            \"test_EM\": 0.9,\n",
    "            \"test_F1\": 0.8,\n",
    "        }\n",
    "        self.test_step_outputs.append(metrics_dict)\n",
    "        return metrics_dict\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        results = {\n",
    "            \"F1\": torch.tensor([x[\"test_EM\"] for x in self.test_step_outputs]).mean(),\n",
    "            \"EM\": torch.tensor([x[\"test_F1\"] for x in self.test_step_outputs]).mean(),\n",
    "        }\n",
    "\n",
    "        for k, v in results.items():\n",
    "            self.log(\n",
    "                f\"test_{k}\",\n",
    "                v,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "                sync_dist=True,\n",
    "            )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        # special scheduler for transformers\n",
    "        optimizer = optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=0.001,  # self.cfg_fitting.learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-08,\n",
    "            weight_decay=0.05,\n",
    "        )\n",
    "\n",
    "        # def lr_scheduler_lambda_1(epoch):\n",
    "        #     if epoch < 3:\n",
    "        #         # warm up lr\n",
    "        #         lr_scale = self.cfg_fitting.lr_rate[epoch]\n",
    "        #     else:\n",
    "        #         # warmup schedule\n",
    "        #         lr_pos = int(\n",
    "        #             -1 - bisect.bisect_left(self.cfg_fitting.lr_scheduler_epoch, epoch)\n",
    "        #         )\n",
    "        #         if lr_pos < -3:\n",
    "        #             lr_scale = max(self.cfg_fitting.lr_rate[0] * (0.98**epoch), 0.03)\n",
    "        #         else:\n",
    "        #             lr_scale = self.cfg_fitting.lr_rate[lr_pos]\n",
    "        #     return lr_scale\n",
    "\n",
    "        # scheduler_1 = optim.lr_scheduler.LambdaLR(\n",
    "        #     optimizer, lr_lambda=lr_scheduler_lambda_1\n",
    "        # )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            # \"lr_scheduler\": scheduler_1,\n",
    "            \"monitor\": \"val_loss\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from nlp import Dataset\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import random\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "# from babl.data import convert_to_features\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "from babl.data import T2TDataCollator\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    ")\n",
    "\n",
    "\n",
    "class CallbackCollection:\n",
    "    def __init__(self, model_name, data_path) -> None:\n",
    "\n",
    "        ####################################################################################\n",
    "        @dataclass\n",
    "        class FittingArgs:\n",
    "            es_patience: int = 5\n",
    "            model_dir: str = str(Path(\"/home/ola/Code/babl/outputs\") / model_name)\n",
    "\n",
    "        ####################################################################################\n",
    "\n",
    "        self.data_path = data_path\n",
    "\n",
    "        self.args = FittingArgs()\n",
    "\n",
    "    def __call__(self):\n",
    "        lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            mode=\"min\", monitor=\"val_loss\", patience=self.args.es_patience\n",
    "        )\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            monitor=\"val_loss\",\n",
    "            dirpath=self.args.model_dir,\n",
    "            save_top_k=2,\n",
    "            save_last=True,\n",
    "            mode=\"min\",\n",
    "            filename=\"{epoch}-{val_loss:.2f}-{val_acc:.2f}-{val_ttr:.2f}-{val_ftr:.2f}\",\n",
    "        )\n",
    "\n",
    "        callbacks = {\n",
    "            \"checkpoint\": checkpoint_callback,\n",
    "            \"lr\": lr_monitor,\n",
    "            \"es\": early_stopping,\n",
    "        }\n",
    "        # callbacks = [checkpoint_callback, lr_monitor, early_stopping]\n",
    "        return callbacks\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dath_path, tokenizer, plain_text=False):\n",
    "        # super().__init__()\n",
    "        self.data_path = Path(dath_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.plain_text = plain_text\n",
    "        self.ds = {}\n",
    "        if plain_text:\n",
    "            self.construct_ds(self.extract_valid_pairs(self.read()))\n",
    "        else:\n",
    "\n",
    "            self.construct_ds(self.extract_valid_pairs(self.read()))\n",
    "            self.ds = self.convert_to_features(self.ds)\n",
    "\n",
    "    def __len__(self):\n",
    "        # print(self.ds)\n",
    "        return list(self.ds.values())[0].__len__()\n",
    "        # return len(self.ds.values()[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.plain_text:\n",
    "            return {\n",
    "                \"input_text\": self.ds[\"input_text\"][idx],\n",
    "                \"target_text\": self.ds[\"target_text\"][idx],\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"input_ids\": self.ds[\"input_ids\"][idx],\n",
    "                \"attention_mask\": self.ds[\"attention_mask\"][idx],\n",
    "                \"target_ids\": self.ds[\"input_ids\"][idx],\n",
    "                \"target_attention_mask\": self.ds[\"attention_mask\"][idx],\n",
    "            }\n",
    "\n",
    "    def read(\n",
    "        self,\n",
    "    ):\n",
    "        examples = []\n",
    "        with open(self.data_path, \"r\") as json_file:\n",
    "            x = list(json_file)\n",
    "            # logger.debug(f\"[data.py::build_dataset]{x=}\")\n",
    "            for json_str in x:\n",
    "                examples.append(json.loads(json_str))\n",
    "        return examples\n",
    "\n",
    "    def extract_valid_pairs(self, samples):\n",
    "        valid_questions = []\n",
    "        for l in samples:\n",
    "            # clear all docs with more or less than one answer\n",
    "            # clean all docs with no annotations\n",
    "            if len(l[\"annotations\"][0][\"short_answers\"]) == 1:\n",
    "                if len(l[\"long_answer_candidates\"]) > 2:\n",
    "                    valid_questions.append(l)\n",
    "        return valid_questions\n",
    "\n",
    "    def construct_ds(self, examples):\n",
    "\n",
    "        self.ds[\"input_text\"] = []\n",
    "        self.ds[\"target_text\"] = []\n",
    "        for i, q in enumerate(examples):\n",
    "\n",
    "            # fitting dataset; positive and negative fitting examples\n",
    "            if random.randint(0, 1) == 1:\n",
    "                # Construct positive example\n",
    "                self.ds[\"input_text\"].append(\n",
    "                    f\"question: {q['question_text']}  context: {self.get_long_answer(q)} </s>\"\n",
    "                )\n",
    "                self.ds[\"target_text\"].append(self.get_short_answer(q))\n",
    "            else:\n",
    "                # Construct negative example\n",
    "                self.ds[\"input_text\"].append(\n",
    "                    f\"question: {q['question_text']}  context: {self.get_random_negative(q)} </s>\"\n",
    "                )\n",
    "                self.ds[\"target_text\"].append(\"None </s>\")\n",
    "        assert len(self.ds[\"target_text\"]) == len(\n",
    "            self.ds[\"input_text\"]\n",
    "        ), \"incorrect data distribution\"\n",
    "\n",
    "    def convert_to_features(self, batch):\n",
    "\n",
    "        # tokenizer = T5Tokenizer.from_pretrained(args.model_name_or_path)  # \"t5-small\")\n",
    "        ####################################################################################\n",
    "        @dataclass\n",
    "        class Args:\n",
    "            input_max_len: int = 64\n",
    "            output_max_len: int = 64\n",
    "\n",
    "            ####################################################################################\n",
    "\n",
    "        input_encodings = self.tokenizer.batch_encode_plus(\n",
    "            batch_text_or_text_pairs=batch[\"input_text\"],\n",
    "            truncation=True,\n",
    "            pad_to_max_length=True,\n",
    "            max_length=Args().input_max_len,\n",
    "        )\n",
    "        target_encodings = self.tokenizer.batch_encode_plus(\n",
    "            batch_text_or_text_pairs=batch[\"target_text\"],\n",
    "            truncation=True,\n",
    "            pad_to_max_length=True,\n",
    "            max_length=Args().output_max_len,\n",
    "        )\n",
    "        # print(\"input_encodings\", input_encodings.keys())\n",
    "        # print(\"target_encodings\", target_encodings.keys())\n",
    "        # encodings = {\n",
    "        #     \"input_ids\": input_encodings[\"input_ids\"],\n",
    "        #     \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "        #     \"target_ids\": target_encodings[\"input_ids\"],\n",
    "        #     \"target_attention_mask\": target_encodings[\"attention_mask\"],\n",
    "        # }\n",
    "\n",
    "        encodings = {\n",
    "            \"input_ids\": torch.tensor(input_encodings[\"input_ids\"]),\n",
    "            \"attention_mask\": torch.tensor(input_encodings[\"attention_mask\"]),\n",
    "            \"target_ids\": torch.tensor(target_encodings[\"input_ids\"]),\n",
    "            \"target_attention_mask\": torch.tensor(target_encodings[\"attention_mask\"]),\n",
    "        }\n",
    "        return encodings\n",
    "\n",
    "    def get_exert(self, doc, start_token, end_token):\n",
    "        return \" \".join(doc.split(\" \")[start_token:end_token])\n",
    "\n",
    "    def get_short_answer(self, q):\n",
    "        answer_indx = q[\"annotations\"][0][\"short_answers\"][0]\n",
    "        return self.get_exert(\n",
    "            q[\"document_text\"], answer_indx[\"start_token\"], answer_indx[\"end_token\"]\n",
    "        )\n",
    "\n",
    "    def get_long_answer(self, q):\n",
    "        answer_indx = q[\"annotations\"][0][\"long_answer\"]\n",
    "        return self.get_exert(\n",
    "            q[\"document_text\"], answer_indx[\"start_token\"], answer_indx[\"end_token\"]\n",
    "        )\n",
    "\n",
    "    def get_random_negative(self, q):\n",
    "        long_answer_indx = q[\"annotations\"][0][\"long_answer\"]\n",
    "\n",
    "        for i in range(len(q[\"long_answer_candidates\"])):\n",
    "            if (\n",
    "                q[\"long_answer_candidates\"][i][\"start_token\"]\n",
    "                == long_answer_indx[\"start_token\"]\n",
    "            ):\n",
    "                del q[\"long_answer_candidates\"][i]\n",
    "                break\n",
    "\n",
    "        answer_indx = random.choice(q[\"long_answer_candidates\"])\n",
    "        return self.get_exert(\n",
    "            q[\"document_text\"], answer_indx[\"start_token\"], answer_indx[\"end_token\"]\n",
    "        )\n",
    "\n",
    "\n",
    "class TextDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path, tokenizer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_path = Path(data_path) / \"50k.jsonl\"\n",
    "        self.val_path = Path(data_path) / \"10k.jsonl\"\n",
    "        # NOTICE, we  re-use the validatio dataset\n",
    "        self.test_path = Path(data_path) / \"10k.jsonl\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pin_memory = False  # True if torch.cuda.is_available() else False\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        ds_train = TextDataset(self.train_path, self.tokenizer)\n",
    "        return DataLoader(\n",
    "            ds_train,\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=T2TDataCollator(),\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        ds_val = TextDataset(self.val_path, self.tokenizer)\n",
    "        return DataLoader(\n",
    "            ds_val,\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=T2TDataCollator(),\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "\n",
    "        ds_test = TextDataset(self.test_path, self.tokenizer)\n",
    "        return DataLoader(\n",
    "            ds_test,\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=T2TDataCollator(),\n",
    "        )\n",
    "\n",
    "\n",
    "# class Predictor(nn.Module):\n",
    "#     def __init__(self, model):\n",
    "#         super().__init__()\n",
    "#         self.model = model\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         logits = self.model(x)\n",
    "#         pred = F.sigmoid(logits)\n",
    "#         return pred\n",
    "\n",
    "\n",
    "class Fitter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        model_name,\n",
    "        data_path=\"/home/ola/Code/babl/inputs\",\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_name = model_name\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def setup(self):\n",
    "        data_module = TextDataModule(data_path=self.data_path, tokenizer=self.tokenizer)\n",
    "\n",
    "        train_loader = data_module.train_dataloader()\n",
    "        val_loader = data_module.val_dataloader()\n",
    "        test_loader = data_module.test_dataloader()\n",
    "\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    def callbacks(self):\n",
    "        # cfg_fitting = self.cfg_fitting\n",
    "        callback_collection = CallbackCollection(self.model_name, self.data_path)\n",
    "        return callback_collection()\n",
    "\n",
    "    def __call__(self):\n",
    "\n",
    "        ####################################################################################\n",
    "        @dataclass\n",
    "        class FittingArgs:\n",
    "            es_patience: int = 5\n",
    "            model_dir: str = str(Path(\"/home/ola/Code/babl/outputs\") / self.model_name)\n",
    "\n",
    "        ####################################################################################\n",
    "\n",
    "        args = FittingArgs()\n",
    "\n",
    "        logger = TensorBoardLogger(\n",
    "            save_dir=args.model_dir,\n",
    "            name=\"lightning_logs\",\n",
    "        )\n",
    "        Model = self.model\n",
    "        # get loaders and datamodule to access input shape\n",
    "        train_loader, val_loader, test_loader = self.setup()\n",
    "        print(\"Created training, validating and test loaders .... \")\n",
    "        # get input shape for onnx exporting\n",
    "        # input_shape = data_module.input_shape\n",
    "        # init model\n",
    "        # kwargs = {}\n",
    "        # model = Model(**kwargs)\n",
    "\n",
    "        # setup training, validating and testing routines for the model\n",
    "        routine = Routine(self.model)\n",
    "\n",
    "        # Init a trainer to execute routine\n",
    "        callback_dict = self.callbacks()\n",
    "        callback_list = [v for (_, v) in callback_dict.items()]\n",
    "        number_devices = os.getenv(\"CUDA_VISIBLE_DEVICES\", \"1,\").split(\",\")\n",
    "        try:\n",
    "            number_devices.remove(\"\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        ####################################################################################\n",
    "        @dataclass\n",
    "        class FittingArgs:\n",
    "            max_epoch: int = 10\n",
    "            fast_dev_run: bool = True\n",
    "\n",
    "        ####################################################################################\n",
    "\n",
    "        args = FittingArgs()\n",
    "        trainer = Trainer(\n",
    "            accelerator=\"gpu\",\n",
    "            devices=len(number_devices),\n",
    "            strategy=os.getenv(\"STRATEGY\", \"ddp_notebook\"),\n",
    "            sync_batchnorm=True,\n",
    "            logger=logger,\n",
    "            max_epochs=args.max_epoch,\n",
    "            callbacks=callback_list,\n",
    "            num_sanity_val_steps=2,\n",
    "            # resume_from_checkpoint=self.cfg_fitting.resume_from_checkpoint,\n",
    "            gradient_clip_val=1.0,\n",
    "            fast_dev_run=args.fast_dev_run,\n",
    "        )\n",
    "\n",
    "        trainer.fit(\n",
    "            routine, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    "        )  # ,ckpt_path=PATH)\n",
    "\n",
    "        if args.fast_dev_run:\n",
    "            # issue with finding best weights path for in fast dev run using last model weights\n",
    "            model_ckpt_path = callback_dict[\"checkpoint\"].__dict__[\"last_model_path\"]\n",
    "        else:\n",
    "            model_ckpt_path = callback_dict[\"checkpoint\"].__dict__[\"best_model_path\"]\n",
    "\n",
    "        trainer.test(\n",
    "            dataloaders=test_loader,\n",
    "            ckpt_path=model_ckpt_path,\n",
    "        )\n",
    "        # Return the input_shapes and trainer of the model for exporting\n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/transformers/models/t5/tokenization_t5.py:289: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created training, validating and test loaders .... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type                       | Params | Mode\n",
      "------------------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60.5 M | eval\n",
      "------------------------------------------------------------\n",
      "60.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 M    Total params\n",
      "242.026   Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "277       Modules in eval mode\n",
      "/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s] keys = dict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_attention_mask'])\n",
      "batch={'input_ids': tensor([[  822,    10,   125,  ...,   376,     3,     1],\n",
      "        [  822,    10,   149,  ...,     9,  9369,     1],\n",
      "        [  822,    10,    46,  ...,  4401,   725,     1],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,    87,   382,     1],\n",
      "        [  822,    10,   125,  ...,   341,  3223,     1],\n",
      "        [  822,    10,   125,  ..., 19282,     3,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'labels': tensor([[  822,    10,   125,  ...,   376,     3,     1],\n",
      "        [  822,    10,   149,  ...,     9,  9369,     1],\n",
      "        [  822,    10,    46,  ...,  4401,   725,     1],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,    87,   382,     1],\n",
      "        [  822,    10,   125,  ...,   341,  3223,     1],\n",
      "        [  822,    10,   125,  ..., 19282,     3,     1]], device='cuda:0'), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/multiprocessing/spawn.py\", line 90, in _wrap\n    fn(i, *args)\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 575, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 982, in _run\n    results = self._run_stage()\n              ^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1026, in _run_stage\n    self.fit_loop.run()\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py\", line 216, in run\n    self.advance()\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py\", line 455, in advance\n    self.epoch_loop.run(self._data_fetcher)\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 150, in run\n    self.advance(data_fetcher)\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 320, in advance\n    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 192, in run\n    self._optimizer_step(batch_idx, closure)\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 270, in _optimizer_step\n    call._call_lightning_module_hook(\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 171, in _call_lightning_module_hook\n    output = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 1302, in optimizer_step\n    optimizer.step(closure=optimizer_closure)\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py\", line 154, in step\n    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/strategies/ddp.py\", line 270, in optimizer_step\n    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 239, in optimizer_step\n    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py\", line 123, in optimizer_step\n    return optimizer.step(closure=closure, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 487, in wrapper\n    out = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n    ret = func(self, *args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/optim/adamw.py\", line 197, in step\n    loss = closure()\n           ^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py\", line 109, in _wrap_closure\n    closure_result = closure()\n                     ^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 146, in __call__\n    self._result = self.closure(*args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 131, in closure\n    step_output = self._step_fn()\n                  ^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 319, in _training_step\n    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 323, in _call_strategy_hook\n    output = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 390, in training_step\n    return self._forward_redirection(self.model, self.lightning_module, \"training_step\", *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 641, in __call__\n    wrapper_output = wrapper_module(*args, **kwargs)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/nn/parallel/distributed.py\", line 1643, in forward\n    else self._run_ddp_forward(*inputs, **kwargs)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/nn/parallel/distributed.py\", line 1459, in _run_ddp_forward\n    return self.module(*inputs, **kwargs)  # type: ignore[index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 634, in wrapped_forward\n    out = method(*_args, **_kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_61659/3088556754.py\", line 30, in training_step\n    y_hat = self(**batch)\n            ^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: Routine.forward() got an unexpected keyword argument 'input_ids'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 27\u001b[0m\n\u001b[1;32m     18\u001b[0m ds \u001b[38;5;241m=\u001b[39m TextDataset(data_path_val, tokenizer\u001b[38;5;241m=\u001b[39mt, plain_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# from babl.data import T2TDataCollator\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# from torch.utils.data import DataLoader\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# t_dl = DataLoader(ds, batch_size=64, shuffle=True, collate_fn=T2TDataCollator())\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# data_module = TextDataModule(data_path, tokenizer)\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mFitter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_path_root\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 365\u001b[0m, in \u001b[0;36mFitter.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m args \u001b[38;5;241m=\u001b[39m FittingArgs()\n\u001b[1;32m    351\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    352\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    353\u001b[0m     devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(number_devices),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    362\u001b[0m     fast_dev_run\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mfast_dev_run,\n\u001b[1;32m    363\u001b[0m )\n\u001b[0;32m--> 365\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroutine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ,ckpt_path=PATH)\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mfast_dev_run:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;66;03m# issue with finding best weights path for in fast dev run using last model weights\u001b[39;00m\n\u001b[1;32m    371\u001b[0m     model_ckpt_path \u001b[38;5;241m=\u001b[39m callback_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_model_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:46\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:144\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m process_context \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mstart_processes(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapping_function,\n\u001b[1;32m    138\u001b[0m     args\u001b[38;5;241m=\u001b[39mprocess_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# we will join ourselves to get the process references\u001b[39;00m\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocs \u001b[38;5;241m=\u001b[39m process_context\u001b[38;5;241m.\u001b[39mprocesses\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mprocess_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    147\u001b[0m worker_output \u001b[38;5;241m=\u001b[39m return_queue\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:203\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    201\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m error_index\n\u001b[1;32m    202\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m original_trace\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[38;5;241m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/multiprocessing/spawn.py\", line 90, in _wrap\n    fn(i, *args)\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 575, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 982, in _run\n    results = self._run_stage()\n              ^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1026, in _run_stage\n    self.fit_loop.run()\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py\", line 216, in run\n    self.advance()\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py\", line 455, in advance\n    self.epoch_loop.run(self._data_fetcher)\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 150, in run\n    self.advance(data_fetcher)\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 320, in advance\n    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 192, in run\n    self._optimizer_step(batch_idx, closure)\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 270, in _optimizer_step\n    call._call_lightning_module_hook(\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 171, in _call_lightning_module_hook\n    output = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 1302, in optimizer_step\n    optimizer.step(closure=optimizer_closure)\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py\", line 154, in step\n    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/strategies/ddp.py\", line 270, in optimizer_step\n    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 239, in optimizer_step\n    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py\", line 123, in optimizer_step\n    return optimizer.step(closure=closure, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 487, in wrapper\n    out = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n    ret = func(self, *args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/optim/adamw.py\", line 197, in step\n    loss = closure()\n           ^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py\", line 109, in _wrap_closure\n    closure_result = closure()\n                     ^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 146, in __call__\n    self._result = self.closure(*args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 131, in closure\n    step_output = self._step_fn()\n                  ^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 319, in _training_step\n    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 323, in _call_strategy_hook\n    output = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 390, in training_step\n    return self._forward_redirection(self.model, self.lightning_module, \"training_step\", *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 641, in __call__\n    wrapper_output = wrapper_module(*args, **kwargs)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/nn/parallel/distributed.py\", line 1643, in forward\n    else self._run_ddp_forward(*inputs, **kwargs)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/nn/parallel/distributed.py\", line 1459, in _run_ddp_forward\n    return self.module(*inputs, **kwargs)  # type: ignore[index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 634, in wrapped_forward\n    out = method(*_args, **_kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_61659/3088556754.py\", line 30, in training_step\n    y_hat = self(**batch)\n            ^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ola/.local/share/virtualenvs/babl-f0iBC2ut/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: Routine.forward() got an unexpected keyword argument 'input_ids'\n"
     ]
    }
   ],
   "source": [
    "from babl.models import MODELS_CHOICES, MODELS\n",
    "from pathlib import Path\n",
    "\n",
    "model_name = \"t5\"\n",
    "full_model_name = MODELS_CHOICES[model_name][0]\n",
    "t_w_m = MODELS[model_name]\n",
    "\n",
    "tokenizer = t_w_m[\"tok\"]\n",
    "model = t_w_m[\"model\"]\n",
    "\n",
    "t = tokenizer.from_pretrained(full_model_name)\n",
    "m = model.from_pretrained(full_model_name)\n",
    "\n",
    "data_path_root = Path(\"/home/ola/Code/babl/inputs\")\n",
    "\n",
    "\n",
    "# data_path_val = data_path_root / \"10k.jsonl\"\n",
    "# ds = TextDataset(data_path_val, tokenizer=t, plain_text=False)\n",
    "# from babl.data import T2TDataCollator\n",
    "# from torch.utils.data import DataLoader\n",
    "# t_dl = DataLoader(ds, batch_size=64, shuffle=True, collate_fn=T2TDataCollator())\n",
    "# test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "# data_module = TextDataModule(data_path, tokenizer)\n",
    "\n",
    "Fitter(model=m, model_name=full_model_name, tokenizer=t, data_path=data_path_root)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  822,    10,   149,  ...,  7365,  8244,     1],\n",
      "        [  822,    10,   125,  ..., 25930,    35,     1],\n",
      "        [  822,    10,   113,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,  2390,   789,     1],\n",
      "        [  822,    10,   116,  ...,     5,   299,     1],\n",
      "        [  822,    10,   116,  ...,    87,   382,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,   149,  ...,  7365,  8244,     1],\n",
      "        [  822,    10,   125,  ..., 25930,    35,     1],\n",
      "        [  822,    10,   113,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,  2390,   789,     1],\n",
      "        [  822,    10,   116,  ...,     5,   299,     1],\n",
      "        [  822,    10,   116,  ...,    87,   382,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  822,    10,   213,  ..., 19628,  2552,     1],\n",
      "        [  822,    10,   116,  ...,     3,     2,     1],\n",
      "        [  822,    10,   125,  ...,    61,    41,     1],\n",
      "        ...,\n",
      "        [  822,    10,   116,  ...,    26,   834,     1],\n",
      "        [  822,    10,   113,  ...,     3,     2,     1],\n",
      "        [  822,    10,   113,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  822,    10,   213,  ..., 19628,  2552,     1],\n",
      "        [  822,    10,   116,  ...,     3,     2,     1],\n",
      "        [  822,    10,   125,  ...,    61,    41,     1],\n",
      "        ...,\n",
      "        [  822,    10,   116,  ...,    26,   834,     1],\n",
      "        [  822,    10,   113,  ...,     3,     2,     1],\n",
      "        [  822,    10,   113,  ...,  -100,  -100,  -100]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([[ 822,   10,  113,  ...,  160,  814,    1],\n",
      "        [ 822,   10,  125,  ..., 2109,   38,    1],\n",
      "        [ 822,   10,  116,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 822,   10,  125,  ...,    6,   44,    1],\n",
      "        [ 822,   10,  125,  ...,    0,    0,    0],\n",
      "        [ 822,   10,    8,  ...,  382,   26,    1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[ 822,   10,  113,  ...,  160,  814,    1],\n",
      "        [ 822,   10,  125,  ..., 2109,   38,    1],\n",
      "        [ 822,   10,  116,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [ 822,   10,  125,  ...,    6,   44,    1],\n",
      "        [ 822,   10,  125,  ..., -100, -100, -100],\n",
      "        [ 822,   10,    8,  ...,  382,   26,    1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[822,  10, 116,  ...,   0,   0,   0],\n",
      "        [822,  10, 113,  ...,   0,   0,   0],\n",
      "        [822,  10, 116,  ...,   4, 553,   1],\n",
      "        ...,\n",
      "        [822,  10, 113,  ...,  23, 157,   1],\n",
      "        [822,  10, 113,  ...,   6, 258,   1],\n",
      "        [822,  10, 113,  ...,   0,   0,   0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ 822,   10,  116,  ..., -100, -100, -100],\n",
      "        [ 822,   10,  113,  ..., -100, -100, -100],\n",
      "        [ 822,   10,  116,  ...,    4,  553,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  113,  ...,   23,  157,    1],\n",
      "        [ 822,   10,  113,  ...,    6,  258,    1],\n",
      "        [ 822,   10,  113,  ..., -100, -100, -100]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([[ 822,   10,  125,  ...,  434,   23,    1],\n",
      "        [ 822,   10,  116,  ...,    7,    7,    1],\n",
      "        [ 822,   10,  116,  ...,  434,   23,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  113,  ..., 3155, 3104,    1],\n",
      "        [ 822,   10,  381,  ...,    0,    0,    0],\n",
      "        [ 822,   10,  113,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ 822,   10,  125,  ...,  434,   23,    1],\n",
      "        [ 822,   10,  116,  ...,    7,    7,    1],\n",
      "        [ 822,   10,  116,  ...,  434,   23,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  113,  ..., 3155, 3104,    1],\n",
      "        [ 822,   10,  381,  ..., -100, -100, -100],\n",
      "        [ 822,   10,  113,  ..., -100, -100, -100]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([[  822,    10,   113,  ...,     0,     0,     0],\n",
      "        [  822,    10,   125,  ...,  5054,     3,     1],\n",
      "        [  822,    10,   125,  ...,  3155,     3,     1],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ..., 13927, 17544,     1],\n",
      "        [  822,    10,   116,  ...,     7,  2837,     1],\n",
      "        [  822,    10,   113,  ...,    47,  1883,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,   113,  ...,  -100,  -100,  -100],\n",
      "        [  822,    10,   125,  ...,  5054,     3,     1],\n",
      "        [  822,    10,   125,  ...,  3155,     3,     1],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ..., 13927, 17544,     1],\n",
      "        [  822,    10,   116,  ...,     7,  2837,     1],\n",
      "        [  822,    10,   113,  ...,    47,  1883,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  822,    10,   116,  ...,     3, 22279,     1],\n",
      "        [  822,    10,   113,  ...,  1162,  2003,     1],\n",
      "        [  822,    10,    84,  ...,     1,     0,     0],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,    54, 18253,     1],\n",
      "        [  822,    10,   113,  ...,     0,     0,     0],\n",
      "        [  822,    10,     8,  ..., 12411,  1109,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,   116,  ...,     3, 22279,     1],\n",
      "        [  822,    10,   113,  ...,  1162,  2003,     1],\n",
      "        [  822,    10,    84,  ...,     1,  -100,  -100],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,    54, 18253,     1],\n",
      "        [  822,    10,   113,  ...,  -100,  -100,  -100],\n",
      "        [  822,    10,     8,  ..., 12411,  1109,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[ 822,   10,   16,  ...,    0,    0,    0],\n",
      "        [ 822,   10,  113,  ..., 4479,   12,    1],\n",
      "        [ 822,   10,  113,  ..., 2239,    3,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  113,  ...,    2, 8991,    1],\n",
      "        [ 822,   10,  125,  ...,  382,   52,    1],\n",
      "        [ 822,   10,  113,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ 822,   10,   16,  ..., -100, -100, -100],\n",
      "        [ 822,   10,  113,  ..., 4479,   12,    1],\n",
      "        [ 822,   10,  113,  ..., 2239,    3,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  113,  ...,    2, 8991,    1],\n",
      "        [ 822,   10,  125,  ...,  382,   52,    1],\n",
      "        [ 822,   10,  113,  ..., -100, -100, -100]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([[ 822,   10,  125,  ..., 3301,    3,    1],\n",
      "        [ 822,   10,  125,  ..., 7960,    7,    1],\n",
      "        [ 822,   10,  116,  ...,  907, 1323,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  113,  ...,    0,    0,    0],\n",
      "        [ 822,   10,  910,  ...,    0,    0,    0],\n",
      "        [ 822,   10,  125,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ 822,   10,  125,  ..., 3301,    3,    1],\n",
      "        [ 822,   10,  125,  ..., 7960,    7,    1],\n",
      "        [ 822,   10,  116,  ...,  907, 1323,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  113,  ..., -100, -100, -100],\n",
      "        [ 822,   10,  910,  ..., -100, -100, -100],\n",
      "        [ 822,   10,  125,  ..., -100, -100, -100]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([[  822,    10,   113,  ...,     3,     2,     1],\n",
      "        [  822,    10,   113,  ...,     3,     4,     1],\n",
      "        [  822,    10,   113,  ..., 17948,     3,     1],\n",
      "        ...,\n",
      "        [  822,    10,   116,  ...,     3,     4,     1],\n",
      "        [  822,    10,    84,  ...,     0,     0,     0],\n",
      "        [  822,    10,   113,  ...,     3,    18,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,   113,  ...,     3,     2,     1],\n",
      "        [  822,    10,   113,  ...,     3,     4,     1],\n",
      "        [  822,    10,   113,  ..., 17948,     3,     1],\n",
      "        ...,\n",
      "        [  822,    10,   116,  ...,     3,     4,     1],\n",
      "        [  822,    10,    84,  ...,  -100,  -100,  -100],\n",
      "        [  822,    10,   113,  ...,     3,    18,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  822,    10,   113,  ...,     0,     0,     0],\n",
      "        [  822,    10,   113,  ...,  6911,     3,     1],\n",
      "        [  822,    10,   116,  ...,  3815, 12191,     1],\n",
      "        ...,\n",
      "        [  822,    10,    46,  ...,    13,   389,     1],\n",
      "        [  822,    10,   113,  ...,     3,    10,     1],\n",
      "        [  822,    10,   116,  ..., 26671,    36,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,   113,  ...,  -100,  -100,  -100],\n",
      "        [  822,    10,   113,  ...,  6911,     3,     1],\n",
      "        [  822,    10,   116,  ...,  3815, 12191,     1],\n",
      "        ...,\n",
      "        [  822,    10,    46,  ...,    13,   389,     1],\n",
      "        [  822,    10,   113,  ...,     3,    10,     1],\n",
      "        [  822,    10,   116,  ..., 26671,    36,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[ 822,   10,  125,  ...,    2, 8991,    1],\n",
      "        [ 822,   10,  116,  ...,    3,    2,    1],\n",
      "        [ 822,   10,  149,  ...,    5,   37,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  149,  ...,  204,   11,    1],\n",
      "        [ 822,   10,  125,  ...,    9, 2711,    1],\n",
      "        [ 822,   10,  113,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ 822,   10,  125,  ...,    2, 8991,    1],\n",
      "        [ 822,   10,  116,  ...,    3,    2,    1],\n",
      "        [ 822,   10,  149,  ...,    5,   37,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  149,  ...,  204,   11,    1],\n",
      "        [ 822,   10,  125,  ...,    9, 2711,    1],\n",
      "        [ 822,   10,  113,  ..., -100, -100, -100]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([[  822,    10,     8,  ...,    16,  5053,     1],\n",
      "        [  822,    10,   213,  ...,   382,    26,     1],\n",
      "        [  822,    10,   113,  ...,    26,  3155,     1],\n",
      "        ...,\n",
      "        [  822,    10,   213,  ...,  2503,    16,     1],\n",
      "        [  822,    10,   125,  ..., 13745,     3,     1],\n",
      "        [  822,    10,   213,  ...,   248, 19121,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,     8,  ...,    16,  5053,     1],\n",
      "        [  822,    10,   213,  ...,   382,    26,     1],\n",
      "        [  822,    10,   113,  ...,    26,  3155,     1],\n",
      "        ...,\n",
      "        [  822,    10,   213,  ...,  2503,    16,     1],\n",
      "        [  822,    10,   125,  ..., 13745,     3,     1],\n",
      "        [  822,    10,   213,  ...,   248, 19121,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  822,    10,   113,  ...,  2042,    52,     1],\n",
      "        [  822,    10,   213,  ...,  8991,  3155,     1],\n",
      "        [  822,    10,   149,  ...,     3,  2168,     1],\n",
      "        ...,\n",
      "        [  822,    10,   684,  ...,     3,     2,     1],\n",
      "        [  822,    10,   113,  ..., 10403,     7,     1],\n",
      "        [  822,    10,   113,  ...,   159, 11563,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,   113,  ...,  2042,    52,     1],\n",
      "        [  822,    10,   213,  ...,  8991,  3155,     1],\n",
      "        [  822,    10,   149,  ...,     3,  2168,     1],\n",
      "        ...,\n",
      "        [  822,    10,   684,  ...,     3,     2,     1],\n",
      "        [  822,    10,   113,  ..., 10403,     7,     1],\n",
      "        [  822,    10,   113,  ...,   159, 11563,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[ 822,   10,  113,  ...,  434,   23,    1],\n",
      "        [ 822,   10,  116,  ..., 1467,    3,    1],\n",
      "        [ 822,   10,  113,  ...,   23, 3155,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  213,  ...,    0,    0,    0],\n",
      "        [ 822,   10,  113,  ...,    6,   34,    1],\n",
      "        [ 822,   10,  116,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ 822,   10,  113,  ...,  434,   23,    1],\n",
      "        [ 822,   10,  116,  ..., 1467,    3,    1],\n",
      "        [ 822,   10,  113,  ...,   23, 3155,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  213,  ..., -100, -100, -100],\n",
      "        [ 822,   10,  113,  ...,    6,   34,    1],\n",
      "        [ 822,   10,  116,  ..., -100, -100, -100]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([[  822,    10,   116,  ...,     0,     0,     0],\n",
      "        [  822,    10,   125,  ..., 15546,  7103,     1],\n",
      "        [  822,    10,   149,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ..., 18011,   138,     1],\n",
      "        [  822,    10,   125,  ...,    19,     3,     1],\n",
      "        [  822,    10,   116,  ...,   834,  3297,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,   116,  ...,  -100,  -100,  -100],\n",
      "        [  822,    10,   125,  ..., 15546,  7103,     1],\n",
      "        [  822,    10,   149,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ..., 18011,   138,     1],\n",
      "        [  822,    10,   125,  ...,    19,     3,     1],\n",
      "        [  822,    10,   116,  ...,   834,  3297,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  822,    10,   149,  ...,     5,     3,     1],\n",
      "        [  822,    10,   113,  ...,    30,   991,     1],\n",
      "        [  822,    10,   116,  ...,    16,   932,     1],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,    26,    26,     1],\n",
      "        [  822,    10,   116,  ...,     3, 10503,     1],\n",
      "        [  822,    10,   113,  ...,   100,   814,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,   149,  ...,     5,     3,     1],\n",
      "        [  822,    10,   113,  ...,    30,   991,     1],\n",
      "        [  822,    10,   116,  ...,    16,   932,     1],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,    26,    26,     1],\n",
      "        [  822,    10,   116,  ...,     3, 10503,     1],\n",
      "        [  822,    10,   113,  ...,   100,   814,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  822,    10,   572,  ...,  8507,  6132,     1],\n",
      "        [  822,    10,   116,  ..., 10432,    63,     1],\n",
      "        [  822,    10,   116,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  822,    10,   213,  ...,     5,  7110,     1],\n",
      "        [  822,    10,   116,  ...,     0,     0,     0],\n",
      "        [  822,    10,   149,  ...,    15,    19,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,   572,  ...,  8507,  6132,     1],\n",
      "        [  822,    10,   116,  ..., 10432,    63,     1],\n",
      "        [  822,    10,   116,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [  822,    10,   213,  ...,     5,  7110,     1],\n",
      "        [  822,    10,   116,  ...,  -100,  -100,  -100],\n",
      "        [  822,    10,   149,  ...,    15,    19,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[ 822,   10,  213,  ...,  163, 7556,    1],\n",
      "        [ 822,   10,  125,  ...,  160,   28,    1],\n",
      "        [ 822,   10, 4245,  ..., 2280,    3,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  125,  ...,  489,  382,    1],\n",
      "        [ 822,   10,  113,  ...,    8, 1699,    1],\n",
      "        [ 822,   10,  113,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ 822,   10,  213,  ...,  163, 7556,    1],\n",
      "        [ 822,   10,  125,  ...,  160,   28,    1],\n",
      "        [ 822,   10, 4245,  ..., 2280,    3,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  125,  ...,  489,  382,    1],\n",
      "        [ 822,   10,  113,  ...,    8, 1699,    1],\n",
      "        [ 822,   10,  113,  ..., -100, -100, -100]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([[ 822,   10,  116,  ...,    0,    0,    0],\n",
      "        [ 822,   10,  116,  ...,   31,   28,    1],\n",
      "        [ 822,   10,  116,  ...,    7, 3715,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  116,  ..., 8480,   16,    1],\n",
      "        [ 822,   10,  113,  ...,    2,   87,    1],\n",
      "        [ 822,   10,  213,  ...,  382,   26,    1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[ 822,   10,  116,  ..., -100, -100, -100],\n",
      "        [ 822,   10,  116,  ...,   31,   28,    1],\n",
      "        [ 822,   10,  116,  ...,    7, 3715,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  116,  ..., 8480,   16,    1],\n",
      "        [ 822,   10,  113,  ...,    2,   87,    1],\n",
      "        [ 822,   10,  213,  ...,  382,   26,    1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  822,    10,   125,  ...,     5,    37,     1],\n",
      "        [  822,    10,   213,  ...,   668,     3,     1],\n",
      "        [  822,    10,   213,  ...,  7807,     3,     1],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,    13, 18076,     1],\n",
      "        [  822,    10,   213,  ...,   797, 10929,     1],\n",
      "        [  822,    10,   113,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  822,    10,   125,  ...,     5,    37,     1],\n",
      "        [  822,    10,   213,  ...,   668,     3,     1],\n",
      "        [  822,    10,   213,  ...,  7807,     3,     1],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,    13, 18076,     1],\n",
      "        [  822,    10,   213,  ...,   797, 10929,     1],\n",
      "        [  822,    10,   113,  ...,  -100,  -100,  -100]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([[  822,    10,   113,  ..., 27332,    12,     1],\n",
      "        [  822,    10,   113,  ...,  5704,    29,     1],\n",
      "        [  822,    10,   149,  ...,  1636,   505,     1],\n",
      "        ...,\n",
      "        [  822,    10,   125,  ...,     6,  1374,     1],\n",
      "        [  822,    10,   113,  ...,     0,     0,     0],\n",
      "        [  822,    10,   213,  ...,    31,    41,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,   113,  ..., 27332,    12,     1],\n",
      "        [  822,    10,   113,  ...,  5704,    29,     1],\n",
      "        [  822,    10,   149,  ...,  1636,   505,     1],\n",
      "        ...,\n",
      "        [  822,    10,   125,  ...,     6,  1374,     1],\n",
      "        [  822,    10,   113,  ...,  -100,  -100,  -100],\n",
      "        [  822,    10,   213,  ...,    31,    41,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  822,    10,  1133,  ..., 16533, 22573,     1],\n",
      "        [  822,    10,   113,  ...,     0,     0,     0],\n",
      "        [  822,    10,   213,  ...,     2,  8991,     1],\n",
      "        ...,\n",
      "        [  822,    10,   213,  ..., 10277,    51,     1],\n",
      "        [  822,    10,     3,  ...,    19,  3923,     1],\n",
      "        [  822,    10,   213,  ...,   586,     3,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,  1133,  ..., 16533, 22573,     1],\n",
      "        [  822,    10,   113,  ...,  -100,  -100,  -100],\n",
      "        [  822,    10,   213,  ...,     2,  8991,     1],\n",
      "        ...,\n",
      "        [  822,    10,   213,  ..., 10277,    51,     1],\n",
      "        [  822,    10,     3,  ...,    19,  3923,     1],\n",
      "        [  822,    10,   213,  ...,   586,     3,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  822,    10,   116,  ...,     7,  2837,     1],\n",
      "        [  822,    10,   113,  ...,  2042, 20350,     1],\n",
      "        [  822,    10,   125,  ...,  8136,     3,     1],\n",
      "        ...,\n",
      "        [  822,    10,   116,  ...,     6,     3,     1],\n",
      "        [  822,    10,   125,  ...,     5,   597,     1],\n",
      "        [  822,    10,   149,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  822,    10,   116,  ...,     7,  2837,     1],\n",
      "        [  822,    10,   113,  ...,  2042, 20350,     1],\n",
      "        [  822,    10,   125,  ...,  8136,     3,     1],\n",
      "        ...,\n",
      "        [  822,    10,   116,  ...,     6,     3,     1],\n",
      "        [  822,    10,   125,  ...,     5,   597,     1],\n",
      "        [  822,    10,   149,  ...,  -100,  -100,  -100]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([[ 822,   10,  113,  ...,   87,  382,    1],\n",
      "        [ 822,   10,  213,  ...,    2, 8991,    1],\n",
      "        [ 822,   10,  113,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 822,   10,  113,  ...,    3,    2,    1],\n",
      "        [ 822,   10,  116,  ...,   57,    3,    1],\n",
      "        [ 822,   10,  113,  ...,    8, 7262,    1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[ 822,   10,  113,  ...,   87,  382,    1],\n",
      "        [ 822,   10,  213,  ...,    2, 8991,    1],\n",
      "        [ 822,   10,  113,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [ 822,   10,  113,  ...,    3,    2,    1],\n",
      "        [ 822,   10,  116,  ...,   57,    3,    1],\n",
      "        [ 822,   10,  113,  ...,    8, 7262,    1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  822,    10,   116,  ...,   382,    52,     1],\n",
      "        [  822,    10,   116,  ...,   117,  2701,     1],\n",
      "        [  822,    10,   113,  ...,     3, 21101,     1],\n",
      "        ...,\n",
      "        [  822,    10,   149,  ...,  3155,     3,     1],\n",
      "        [  822,    10,   113,  ...,    49,    13,     1],\n",
      "        [  822,    10,   113,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  822,    10,   116,  ...,   382,    52,     1],\n",
      "        [  822,    10,   116,  ...,   117,  2701,     1],\n",
      "        [  822,    10,   113,  ...,     3, 21101,     1],\n",
      "        ...,\n",
      "        [  822,    10,   149,  ...,  3155,     3,     1],\n",
      "        [  822,    10,   113,  ...,    49,    13,     1],\n",
      "        [  822,    10,   113,  ...,  -100,  -100,  -100]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([[  822,    10,   113,  ...,  3155,   412,     1],\n",
      "        [  822,    10,   113,  ...,    47,  5899,     1],\n",
      "        [  822,    10,   213,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,     0,     0,     0],\n",
      "        [  822,    10,   213,  ...,    17, 11547,     1],\n",
      "        [  822,    10,   113,  ...,  1079,   272,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,   113,  ...,  3155,   412,     1],\n",
      "        [  822,    10,   113,  ...,    47,  5899,     1],\n",
      "        [  822,    10,   213,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,  -100,  -100,  -100],\n",
      "        [  822,    10,   213,  ...,    17, 11547,     1],\n",
      "        [  822,    10,   113,  ...,  1079,   272,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  822,    10,   116,  ...,    46, 20135,     1],\n",
      "        [  822,    10,   572,  ...,     6,    11,     1],\n",
      "        [  822,    10,    46,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,  3155,   419,     1],\n",
      "        [  822,    10,   113,  ...,     0,     0,     0],\n",
      "        [  822,    10,   125,  ...,     3,     5,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,   116,  ...,    46, 20135,     1],\n",
      "        [  822,    10,   572,  ...,     6,    11,     1],\n",
      "        [  822,    10,    46,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,  3155,   419,     1],\n",
      "        [  822,    10,   113,  ...,  -100,  -100,  -100],\n",
      "        [  822,    10,   125,  ...,     3,     5,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[822,  10, 113,  ...,   6,   3,   1],\n",
      "        [822,  10,   3,  ..., 382,  26,   1],\n",
      "        [822,  10, 113,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [822,  10, 125,  ...,   8, 335,   1],\n",
      "        [822,  10, 116,  ...,  87, 434,   1],\n",
      "        [822,  10, 113,  ...,   8, 511,   1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[ 822,   10,  113,  ...,    6,    3,    1],\n",
      "        [ 822,   10,    3,  ...,  382,   26,    1],\n",
      "        [ 822,   10,  113,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [ 822,   10,  125,  ...,    8,  335,    1],\n",
      "        [ 822,   10,  116,  ...,   87,  434,    1],\n",
      "        [ 822,   10,  113,  ...,    8,  511,    1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  822,    10,   125,  ...,    38,  3059,     1],\n",
      "        [  822,    10,   113,  ..., 30756,   894,     1],\n",
      "        [  822,    10,   113,  ...,    11,  1453,     1],\n",
      "        ...,\n",
      "        [  822,    10,  3640,  ...,     8,     3,     1],\n",
      "        [  822,    10,   113,  ...,    34,     3,     1],\n",
      "        [  822,    10,     3,  ...,    13,  5281,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,   125,  ...,    38,  3059,     1],\n",
      "        [  822,    10,   113,  ..., 30756,   894,     1],\n",
      "        [  822,    10,   113,  ...,    11,  1453,     1],\n",
      "        ...,\n",
      "        [  822,    10,  3640,  ...,     8,     3,     1],\n",
      "        [  822,    10,   113,  ...,    34,     3,     1],\n",
      "        [  822,    10,     3,  ...,    13,  5281,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  822,    10,   116,  ...,  1246,   113,     1],\n",
      "        [  822,    10,   116,  ...,  6886, 31829,     1],\n",
      "        [  822,    10,   149,  ...,   506, 13987,     1],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ..., 29421,    23,     1],\n",
      "        [  822,    10,   113,  ...,   117,   305,     1],\n",
      "        [  822,    10,   113,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  822,    10,   116,  ...,  1246,   113,     1],\n",
      "        [  822,    10,   116,  ...,  6886, 31829,     1],\n",
      "        [  822,    10,   149,  ...,   506, 13987,     1],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ..., 29421,    23,     1],\n",
      "        [  822,    10,   113,  ...,   117,   305,     1],\n",
      "        [  822,    10,   113,  ...,  -100,  -100,  -100]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([[  822,    10,   113,  ...,     8, 14783,     1],\n",
      "        [  822,    10,   669,  ...,  3879,     3,     1],\n",
      "        [  822,    10,   213,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,  2457, 11699,     1],\n",
      "        [  822,    10,   113,  ...,   968,     3,     1],\n",
      "        [  822,    10,   336,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  822,    10,   113,  ...,     8, 14783,     1],\n",
      "        [  822,    10,   669,  ...,  3879,     3,     1],\n",
      "        [  822,    10,   213,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,  2457, 11699,     1],\n",
      "        [  822,    10,   113,  ...,   968,     3,     1],\n",
      "        [  822,    10,   336,  ...,  -100,  -100,  -100]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([[ 822,   10,  116,  ...,   23, 3155,    1],\n",
      "        [ 822,   10,  113,  ...,    2,   87,    1],\n",
      "        [ 822,   10,  113,  ...,   70,  293,    1],\n",
      "        ...,\n",
      "        [ 822,   10,   54,  ...,    3,    2,    1],\n",
      "        [ 822,   10,  213,  ...,   19,   16,    1],\n",
      "        [ 822,   10,  125,  ...,    3,    2,    1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[ 822,   10,  116,  ...,   23, 3155,    1],\n",
      "        [ 822,   10,  113,  ...,    2,   87,    1],\n",
      "        [ 822,   10,  113,  ...,   70,  293,    1],\n",
      "        ...,\n",
      "        [ 822,   10,   54,  ...,    3,    2,    1],\n",
      "        [ 822,   10,  213,  ...,   19,   16,    1],\n",
      "        [ 822,   10,  125,  ...,    3,    2,    1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  822,    10,   113,  ...,     0,     0,     0],\n",
      "        [  822,    10,   213,  ...,  2741,   338,     1],\n",
      "        [  822,    10,     3,  ...,     3,     5,     1],\n",
      "        ...,\n",
      "        [  822,    10,   149,  ...,  8991,  3155,     1],\n",
      "        [  822,    10,   125,  ...,     5,    94,     1],\n",
      "        [  822,    10,  1634,  ...,   398, 15950,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,   113,  ...,  -100,  -100,  -100],\n",
      "        [  822,    10,   213,  ...,  2741,   338,     1],\n",
      "        [  822,    10,     3,  ...,     3,     5,     1],\n",
      "        ...,\n",
      "        [  822,    10,   149,  ...,  8991,  3155,     1],\n",
      "        [  822,    10,   125,  ...,     5,    94,     1],\n",
      "        [  822,    10,  1634,  ...,   398, 15950,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[ 822,   10,  125,  ...,   57, 2146,    1],\n",
      "        [ 822,   10,  116,  ...,  121, 3155,    1],\n",
      "        [ 822,   10,  125,  ...,    3,    2,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  113,  ...,   36,    8,    1],\n",
      "        [ 822,   10,  125,  ...,    0,    0,    0],\n",
      "        [ 822,   10,  113,  ..., 3155,    3,    1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[ 822,   10,  125,  ...,   57, 2146,    1],\n",
      "        [ 822,   10,  116,  ...,  121, 3155,    1],\n",
      "        [ 822,   10,  125,  ...,    3,    2,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  113,  ...,   36,    8,    1],\n",
      "        [ 822,   10,  125,  ..., -100, -100, -100],\n",
      "        [ 822,   10,  113,  ..., 3155,    3,    1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[ 822,   10,  113,  ...,    7,   38,    1],\n",
      "        [ 822,   10,  116,  ...,   11,    3,    1],\n",
      "        [ 822,   10,  213,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 822,   10,  116,  ..., 5116,    7,    1],\n",
      "        [ 822,   10,  213,  ..., 7249,  433,    1],\n",
      "        [ 822,   10,  113,  ...,   26, 3155,    1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[ 822,   10,  113,  ...,    7,   38,    1],\n",
      "        [ 822,   10,  116,  ...,   11,    3,    1],\n",
      "        [ 822,   10,  213,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [ 822,   10,  116,  ..., 5116,    7,    1],\n",
      "        [ 822,   10,  213,  ..., 7249,  433,    1],\n",
      "        [ 822,   10,  113,  ...,   26, 3155,    1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  822,    10,     8,  ...,     2,    87,     1],\n",
      "        [  822,    10,   125,  ...,     6,    84,     1],\n",
      "        [  822,    10,   116,  ...,     8,   166,     1],\n",
      "        ...,\n",
      "        [  822,    10,   116,  ..., 22487,  2009,     1],\n",
      "        [  822,    10,   213,  ...,     6,   368,     1],\n",
      "        [  822,    10,   116,  ...,    31,    31,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,     8,  ...,     2,    87,     1],\n",
      "        [  822,    10,   125,  ...,     6,    84,     1],\n",
      "        [  822,    10,   116,  ...,     8,   166,     1],\n",
      "        ...,\n",
      "        [  822,    10,   116,  ..., 22487,  2009,     1],\n",
      "        [  822,    10,   213,  ...,     6,   368,     1],\n",
      "        [  822,    10,   116,  ...,    31,    31,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[ 822,   10,  116,  ...,    0,    0,    0],\n",
      "        [ 822,   10,  149,  ...,    5,   94,    1],\n",
      "        [ 822,   10,  116,  ...,   12,   43,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  213,  ...,    3,    5,    1],\n",
      "        [ 822,   10,  149,  ..., 1980,  173,    1],\n",
      "        [ 822,   10,  125,  ...,   24,   33,    1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[ 822,   10,  116,  ..., -100, -100, -100],\n",
      "        [ 822,   10,  149,  ...,    5,   94,    1],\n",
      "        [ 822,   10,  116,  ...,   12,   43,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  213,  ...,    3,    5,    1],\n",
      "        [ 822,   10,  149,  ..., 1980,  173,    1],\n",
      "        [ 822,   10,  125,  ...,   24,   33,    1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  822,    10,   116,  ...,  3739,  7562,     1],\n",
      "        [  822,    10,   113,  ..., 11679,     3,     1],\n",
      "        [  822,    10,   113,  ...,  3086,  6580,     1],\n",
      "        ...,\n",
      "        [  822,    10,   149,  ...,    31,    31,     1],\n",
      "        [  822,    10,   113,  ..., 15676,     3,     1],\n",
      "        [  822,    10,   116,  ...,   290,   130,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,   116,  ...,  3739,  7562,     1],\n",
      "        [  822,    10,   113,  ..., 11679,     3,     1],\n",
      "        [  822,    10,   113,  ...,  3086,  6580,     1],\n",
      "        ...,\n",
      "        [  822,    10,   149,  ...,    31,    31,     1],\n",
      "        [  822,    10,   113,  ..., 15676,     3,     1],\n",
      "        [  822,    10,   116,  ...,   290,   130,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[ 822,   10,  149,  ..., 2280,    3,    1],\n",
      "        [ 822,   10,  213,  ...,    0,    0,    0],\n",
      "        [ 822,   10,  213,  ...,   31,    7,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  116,  ..., 3155,    3,    1],\n",
      "        [ 822,   10,  113,  ...,    0,    0,    0],\n",
      "        [ 822,   10,  113,  ..., 4143,    3,    1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[ 822,   10,  149,  ..., 2280,    3,    1],\n",
      "        [ 822,   10,  213,  ..., -100, -100, -100],\n",
      "        [ 822,   10,  213,  ...,   31,    7,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  116,  ..., 3155,    3,    1],\n",
      "        [ 822,   10,  113,  ..., -100, -100, -100],\n",
      "        [ 822,   10,  113,  ..., 4143,    3,    1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  822,    10,   116,  ...,    26,   386,     1],\n",
      "        [  822,    10,  1783,  ...,     3,     5,     1],\n",
      "        [  822,    10,   116,  ...,  3297,     7,     1],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,    23,  3155,     1],\n",
      "        [  822,    10, 10209,  ...,     8,   412,     1],\n",
      "        [  822,    10,   213,  ...,    53,  3161,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,   116,  ...,    26,   386,     1],\n",
      "        [  822,    10,  1783,  ...,     3,     5,     1],\n",
      "        [  822,    10,   116,  ...,  3297,     7,     1],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,    23,  3155,     1],\n",
      "        [  822,    10, 10209,  ...,     8,   412,     1],\n",
      "        [  822,    10,   213,  ...,    53,  3161,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  822,    10,   113,  ...,     0,     0,     0],\n",
      "        [  822,    10,   116,  ...,     0,     0,     0],\n",
      "        [  822,    10,   113,  ...,  3155, 14298,     1],\n",
      "        ...,\n",
      "        [  822,    10,   213,  ...,  1530,  2583,     1],\n",
      "        [  822,    10,   125,  ...,     0,     0,     0],\n",
      "        [  822,    10,   113,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  822,    10,   113,  ...,  -100,  -100,  -100],\n",
      "        [  822,    10,   116,  ...,  -100,  -100,  -100],\n",
      "        [  822,    10,   113,  ...,  3155, 14298,     1],\n",
      "        ...,\n",
      "        [  822,    10,   213,  ...,  1530,  2583,     1],\n",
      "        [  822,    10,   125,  ...,  -100,  -100,  -100],\n",
      "        [  822,    10,   113,  ...,  -100,  -100,  -100]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([[  822,    10,   149,  ...,  5490, 15726,     1],\n",
      "        [  822,    10,   113,  ...,   560,     8,     1],\n",
      "        [  822,    10,   116,  ...,    11,     3,     1],\n",
      "        ...,\n",
      "        [  822,    10,   116,  ...,     0,     0,     0],\n",
      "        [  822,    10,   116,  ...,     3,     6,     1],\n",
      "        [  822,    10,   125,  ...,   394,   801,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,   149,  ...,  5490, 15726,     1],\n",
      "        [  822,    10,   113,  ...,   560,     8,     1],\n",
      "        [  822,    10,   116,  ...,    11,     3,     1],\n",
      "        ...,\n",
      "        [  822,    10,   116,  ...,  -100,  -100,  -100],\n",
      "        [  822,    10,   116,  ...,     3,     6,     1],\n",
      "        [  822,    10,   125,  ...,   394,   801,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  822,    10,   113,  ...,     0,     0,     0],\n",
      "        [  822,    10,   213,  ...,     6,  1244,     1],\n",
      "        [  822,    10,   149,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,     0,     0,     0],\n",
      "        [  822,    10,   125,  ..., 16118,    52,     1],\n",
      "        [  822,    10,   113,  ...,     3,   117,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,   113,  ...,  -100,  -100,  -100],\n",
      "        [  822,    10,   213,  ...,     6,  1244,     1],\n",
      "        [  822,    10,   149,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,  -100,  -100,  -100],\n",
      "        [  822,    10,   125,  ..., 16118,    52,     1],\n",
      "        [  822,    10,   113,  ...,     3,   117,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  822,    10,   116,  ...,   415,  2197,     1],\n",
      "        [  822,    10,    84,  ...,     0,     0,     0],\n",
      "        [  822,    10,   213,  ...,  1281, 18126,     1],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,     0,     0,     0],\n",
      "        [  822,    10,    84,  ...,     3,     5,     1],\n",
      "        [  822,    10,   113,  ...,     3,    31,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  822,    10,   116,  ...,   415,  2197,     1],\n",
      "        [  822,    10,    84,  ...,  -100,  -100,  -100],\n",
      "        [  822,    10,   213,  ...,  1281, 18126,     1],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,  -100,  -100,  -100],\n",
      "        [  822,    10,    84,  ...,     3,     5,     1],\n",
      "        [  822,    10,   113,  ...,     3,    31,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[ 822,   10,  116,  ..., 3155,    3,    1],\n",
      "        [ 822,   10,    3,  ...,    3,    2,    1],\n",
      "        [ 822,   10,  213,  ...,   88,  805,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  113,  ..., 6721,    7,    1],\n",
      "        [ 822,   10,  113,  ...,  103,   59,    1],\n",
      "        [ 822,   10,  113,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ 822,   10,  116,  ..., 3155,    3,    1],\n",
      "        [ 822,   10,    3,  ...,    3,    2,    1],\n",
      "        [ 822,   10,  213,  ...,   88,  805,    1],\n",
      "        ...,\n",
      "        [ 822,   10,  113,  ..., 6721,    7,    1],\n",
      "        [ 822,   10,  113,  ...,  103,   59,    1],\n",
      "        [ 822,   10,  113,  ..., -100, -100, -100]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([[ 822,   10,  149,  ..., 7940,    3,    1],\n",
      "        [ 822,   10,  149,  ...,  382,   26,    1],\n",
      "        [ 822,   10,  213,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 822,   10,  113,  ...,  209,    3,    1],\n",
      "        [ 822,   10,  116,  ..., 3155,    1,    0],\n",
      "        [ 822,   10,  113,  ...,    2,  382,    1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[ 822,   10,  149,  ..., 7940,    3,    1],\n",
      "        [ 822,   10,  149,  ...,  382,   26,    1],\n",
      "        [ 822,   10,  213,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [ 822,   10,  113,  ...,  209,    3,    1],\n",
      "        [ 822,   10,  116,  ..., 3155,    1, -100],\n",
      "        [ 822,   10,  113,  ...,    2,  382,    1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([[822,  10, 113,  ...,  87, 345,   1],\n",
      "        [822,  10, 116,  ...,   7, 301,   1],\n",
      "        [822,  10,   3,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [822,  10, 113,  ...,   3,   2,   1],\n",
      "        [822,  10, 116,  ...,   0,   0,   0],\n",
      "        [822,  10, 213,  ...,   0,   0,   0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ 822,   10,  113,  ...,   87,  345,    1],\n",
      "        [ 822,   10,  116,  ...,    7,  301,    1],\n",
      "        [ 822,   10,    3,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [ 822,   10,  113,  ...,    3,    2,    1],\n",
      "        [ 822,   10,  116,  ..., -100, -100, -100],\n",
      "        [ 822,   10,  213,  ..., -100, -100, -100]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([[  822,    10,   125,  ..., 17805,     3,     1],\n",
      "        [  822,    10,   116,  ...,     8, 12601,     1],\n",
      "        [  822,    10,   213,  ..., 19281,    29,     1],\n",
      "        ...,\n",
      "        [  822,    10,   125,  ...,  2648,    11,     1],\n",
      "        [  822,    10,   113,  ...,  4922,    51,     1],\n",
      "        [  822,    10,   113,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  822,    10,   125,  ..., 17805,     3,     1],\n",
      "        [  822,    10,   116,  ...,     8, 12601,     1],\n",
      "        [  822,    10,   213,  ..., 19281,    29,     1],\n",
      "        ...,\n",
      "        [  822,    10,   125,  ...,  2648,    11,     1],\n",
      "        [  822,    10,   113,  ...,  4922,    51,     1],\n",
      "        [  822,    10,   113,  ...,  -100,  -100,  -100]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([[  822,    10,   149,  ...,   434,    23,     1],\n",
      "        [  822,    10,    19,  ...,     0,     0,     0],\n",
      "        [  822,    10,   113,  ..., 17461,    65,     1],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,     2,   382,     1],\n",
      "        [  822,    10,   213,  ...,    16,   728,     1],\n",
      "        [  822,    10,   125,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  822,    10,   149,  ...,   434,    23,     1],\n",
      "        [  822,    10,    19,  ...,  -100,  -100,  -100],\n",
      "        [  822,    10,   113,  ..., 17461,    65,     1],\n",
      "        ...,\n",
      "        [  822,    10,   113,  ...,     2,   382,     1],\n",
      "        [  822,    10,   213,  ...,    16,   728,     1],\n",
      "        [  822,    10,   125,  ...,  -100,  -100,  -100]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for d in t_dl:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ds[\"input_text\"].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list({\"x\": [1, 2, 3, 4]}.values())[0].__len__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babl-f0iBC2ut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
