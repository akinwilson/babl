{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to pytorch lightning fitting routine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorboard\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 32128\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = 1e-3\n",
    "        self.validation_step_outputs = []\n",
    "        self.training_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        labels,\n",
    "        decoder_attention_mask, \n",
    "    ):\n",
    "        y_hat = self.model(input_ids=input_ids, attention_mask=attention_mask, decoder_attention_mask=decoder_attention_mask, labels=labels)\n",
    "        \n",
    "        # print(f\"forward(): {y_hat=}\")\n",
    "        return y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # print(f\"keys = {batch.keys()}\")\n",
    "        # print(f\"{batch=}\")\n",
    "        y = batch['labels']\n",
    "        y_hat = self(**batch)\n",
    "        y_onehot = F.one_hot(y, num_classes=VOCAB_SIZE)\n",
    "        y = y_onehot.float()\n",
    "        losses = []\n",
    "        # computing cross-entropy on per-token basis and averaging the loss. \n",
    "        for tok in range(y_hat.logits.shape[1]):\n",
    "            # print(\"Per-token loss cross entropy\")\n",
    "            loss = F.cross_entropy(y_hat.logits[:,tok,:] , y[:,tok,:])\n",
    "            # print(loss)\n",
    "            # loss = F.nll_loss(y_hat[:,tok,:] , y[:,tok,:])\n",
    "            losses.append(loss)\n",
    "\n",
    "        loss  = torch.tensor(losses,  requires_grad=True).mean()\n",
    "        # dummy metrics\n",
    "\n",
    "\n",
    "        # calculating exact matches \n",
    "        y_hat = F.softmax(y_hat.logits, dim=-1)\n",
    "        a = y_hat.argmax(1)\n",
    "        y_hat = torch.zeros(y_hat.shape).scatter(1, a.unsqueeze(1), 1.0)\n",
    "        # y_onehot = F.one_hot(y, num_classes=VOCAB_SIZE)\n",
    "        # y = y_onehot.float()\n",
    "        matches = (y == y_hat).int()\n",
    "        correct = matches.sum()\n",
    "        tot = torch.prod(torch.tensor(matches.shape))\n",
    "        metrics_dict = {\"loss\": loss, \"train_EM\": (correct/tot).item(), \"train_F1\": 0.9}\n",
    "        # print(metrics_dict)\n",
    "        self.training_step_outputs.append(metrics_dict)\n",
    "        return metrics_dict\n",
    "\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        results = {\n",
    "            \"loss\": torch.tensor(\n",
    "                [x[\"loss\"] for x in self.training_step_outputs]\n",
    "            ).mean(),\n",
    "            \"F1\": torch.tensor(\n",
    "                [x[\"train_F1\"] for x in self.training_step_outputs]\n",
    "            ).mean(),\n",
    "            \"EM\": torch.tensor(\n",
    "                [x[\"train_EM\"] for x in self.training_step_outputs]\n",
    "            ).mean(),\n",
    "        }\n",
    "        # self.log(f\"LR\",self.lr, on_epoch=True, prog_bar=True, logger=True)\n",
    "        for k, v in results.items():\n",
    "            self.log(\n",
    "                f\"train_{k}\",\n",
    "                v,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "                sync_dist=True,\n",
    "            )\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y = batch['labels']\n",
    "        y_hat = self(**batch)\n",
    "        y_onehot = F.one_hot(y, num_classes=VOCAB_SIZE)\n",
    "        y = y_onehot.float()\n",
    "        losses = []\n",
    "        # computing cross-entropy on per-token basis and averaging the loss. \n",
    "        for tok in range(y_hat.logits.shape[1]):\n",
    "            loss = F.cross_entropy(y_hat.logits[:,tok,:] , y[:,tok,:])\n",
    "            # print(loss)\n",
    "            losses.append(loss)\n",
    "        loss  = torch.tensor(losses).mean()\n",
    "\n",
    "\n",
    "\n",
    "        # calculating exact matches \n",
    "        y_hat = F.softmax(y_hat.logits, dim=-1)\n",
    "        a = y_hat.argmax(1)\n",
    "        y_hat = torch.zeros(y_hat.shape).scatter(1, a.unsqueeze(1), 1.0)\n",
    "        matches = (y == y_hat).int()\n",
    "        correct = matches.sum()\n",
    "        tot = torch.prod(torch.tensor(matches.shape))\n",
    "        \n",
    "\n",
    "\n",
    "        # dummy metrics\n",
    "        metrics_dict = {\"val_loss\": loss.item(), \"val_EM\": (correct/tot).item(), \"val_F1\": 0.9}\n",
    "        self.validation_step_outputs.append(metrics_dict)\n",
    "        return metrics_dict\n",
    "\n",
    "\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        results = {\n",
    "            \"loss\": torch.tensor(\n",
    "                [x[\"val_loss\"] for x in self.validation_step_outputs]\n",
    "            ).mean(),\n",
    "            \"EM\": torch.tensor(\n",
    "                [x[\"val_EM\"] for x in self.validation_step_outputs]\n",
    "            ).mean(),\n",
    "            \"F1\": torch.tensor(\n",
    "                [x[\"val_F1\"] for x in self.validation_step_outputs]\n",
    "            ).mean(),\n",
    "        }\n",
    "        for k, v in results.items():\n",
    "            self.log(\n",
    "                f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True, sync_dist=True\n",
    "            )\n",
    "            # self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True) # , logger=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # x = batch[\"x\"]\n",
    "        \n",
    "        \n",
    "        y = batch[\"labels\"]\n",
    "        y_hat = self(**batch)\n",
    "        \n",
    "        # calculating exact matches \n",
    "        y_hat = F.softmax(y_hat.logits, dim=-1)\n",
    "        a = y_hat.argmax(1)\n",
    "        y_hat = torch.zeros(y_hat.shape).scatter(1, a.unsqueeze(1), 1.0)\n",
    "        y_onehot = F.one_hot(y, num_classes=VOCAB_SIZE)\n",
    "        y = y_onehot.float()\n",
    "        matches = (y == y_hat).int()\n",
    "        correct = matches.sum()\n",
    "        tot = torch.prod(torch.tensor(matches.shape))\n",
    "        \n",
    "        \n",
    "        metrics_dict = {\n",
    "            \"test_EM\": (correct/tot).item(),\n",
    "            \"test_F1\": 0.8,\n",
    "        }\n",
    "        self.test_step_outputs.append(metrics_dict)\n",
    "        return metrics_dict\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        results = {\n",
    "            \"F1\": torch.tensor([x[\"test_F1\"] for x in self.test_step_outputs]).mean(),\n",
    "            \"EM\": torch.tensor([x[\"test_EM\"] for x in self.test_step_outputs]).mean(),\n",
    "        }\n",
    "\n",
    "        for k, v in results.items():\n",
    "            self.log(\n",
    "                f\"test_{k}\",\n",
    "                v,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "                sync_dist=True,\n",
    "            )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        # special scheduler for transformers\n",
    "        optimizer = optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=0.001,  # self.cfg_fitting.learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-08,\n",
    "            weight_decay=0.05,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"monitor\": \"val_loss\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from babl.data import TextDataset, TextDataModule\n",
    "from babl.utils import CallbackCollection\n",
    "\n",
    "class Fitter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        model_name,\n",
    "        data_path=\"../inputs\",\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_name = model_name\n",
    "        self.data_path = data_path\n",
    "\n",
    "        ####################################################################################\n",
    "        @dataclass\n",
    "        class FittingArgs:\n",
    "            es_patience: int = 5\n",
    "            model_dir = Path(\"/home/nameduser/Code/babl/outputs\") / model_name\n",
    "            max_epoch: int = 10\n",
    "            fast_dev_run: bool = False\n",
    "\n",
    "            def __post_init__(self):\n",
    "                self.model_dir.mkdir(parents=True, exist_ok=True)\n",
    "                self.model_dir  =  str(self.model_dir)\n",
    "\n",
    "        ####################################################################################\n",
    "\n",
    "        self.args = FittingArgs()\n",
    "\n",
    "    def setup(self):\n",
    "        data_module = TextDataModule(data_path=self.data_path, tokenizer=self.tokenizer, dev_run=True)\n",
    "\n",
    "        train_loader = data_module.train_dataloader()\n",
    "        val_loader = data_module.val_dataloader()\n",
    "        test_loader = data_module.test_dataloader()\n",
    "\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    def callbacks(self):\n",
    "        # cfg_fitting = self.cfg_fitting\n",
    "        callback_collection = CallbackCollection(self.data_path, self.args)\n",
    "        return callback_collection()\n",
    "\n",
    "    def __call__(self):\n",
    "\n",
    "        logger = TensorBoardLogger(\n",
    "            save_dir=self.args.model_dir,\n",
    "            name=\"lightning_logs\",\n",
    "        )\n",
    "        Model = self.model\n",
    "        # get loaders and datamodule to access input shape\n",
    "        train_loader, val_loader, test_loader = self.setup()\n",
    "        print(\"Created training, validating and test loaders .... \")\n",
    "        # get input shape for onnx exporting\n",
    "        # input_shape = data_module.input_shape\n",
    "        # init model\n",
    "        # kwargs = {}\n",
    "        # model = Model(**kwargs)\n",
    "\n",
    "        # setup training, validating and testing routines for the model\n",
    "        routine = Routine(self.model)\n",
    "\n",
    "        # Init a trainer to execute routine\n",
    "        callback_dict = self.callbacks()\n",
    "        callback_list = [v for (_, v) in callback_dict.items()]\n",
    "        number_devices = os.getenv(\"CUDA_VISIBLE_DEVICES\", \"1,\").split(\",\")\n",
    "        try:\n",
    "            number_devices.remove(\"\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        trainer = Trainer(\n",
    "            accelerator=\"cpu\",\n",
    "            devices=len(number_devices),\n",
    "            # strategy=os.getenv(\"STRATEGY\", \"ddp_notebook\"),\n",
    "            sync_batchnorm=True,\n",
    "            logger=logger,\n",
    "            max_epochs=self.args.max_epoch,\n",
    "            callbacks=callback_list,\n",
    "            num_sanity_val_steps=2,\n",
    "            # resume_from_checkpoint=self.cfg_fitting.resume_from_checkpoint,\n",
    "            gradient_clip_val=1.0,\n",
    "            fast_dev_run=self.args.fast_dev_run,\n",
    "        )\n",
    "\n",
    "        trainer.fit(\n",
    "            routine, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    "        )  # ,ckpt_path=PATH)\n",
    "\n",
    "        if self.args.fast_dev_run:\n",
    "            # issue with finding best weights path for in fast dev run using last model weights\n",
    "            model_ckpt_path = callback_dict[\"checkpoint\"].__dict__[\"last_model_path\"]\n",
    "        else:\n",
    "            model_ckpt_path = callback_dict[\"checkpoint\"].__dict__[\"best_model_path\"]\n",
    "\n",
    "        trainer.test(\n",
    "            dataloaders=test_loader,\n",
    "            ckpt_path=model_ckpt_path,\n",
    "        )\n",
    "        # Return the input_shapes and trainer of the model for exporting\n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV RUN?: True. Using 128 datapoints for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nameduser/.local/share/virtualenvs/babl-qoEDH0A2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/nameduser/.local/share/virtualenvs/babl-qoEDH0A2/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:289: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV RUN?: True. Using 128 datapoints for training\n",
      "DEV RUN?: True. Using 128 datapoints for training\n",
      "Created training, validating and test loaders .... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/nameduser/.local/share/virtualenvs/babl-qoEDH0A2/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/home/nameduser/.local/share/virtualenvs/babl-qoEDH0A2/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/nameduser/Code/babl/outputs/t5-small exists and is not empty.\n",
      "\n",
      "  | Name  | Type                       | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60.5 M | train\n",
      "-------------------------------------------------------------\n",
      "60.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 M    Total params\n",
      "242.026   Total estimated model params size (MB)\n",
      "277       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4940419e624bf99b740171e2253832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nameduser/.local/share/virtualenvs/babl-qoEDH0A2/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "/home/nameduser/.local/share/virtualenvs/babl-qoEDH0A2/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (16) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0cda2cdb5144788970df31b3c0ef361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf22b4001a444296bf635db286d7b5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a228f2fb9ae4545a0917fbd7b422d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215b8788246b4788aec5c4d5390e37c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0f655f4c40469b80ce48a1765cb58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5186c29bfcd44013ac2fd57e5ff4b718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ace2c664644e778b7baed1ac98bf61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/nameduser/Code/babl/outputs/t5-small/epoch=0-val_loss=10.70-val_EM=0.98-val_F1=0.90.ckpt\n",
      "Loaded model weights from the checkpoint at /home/nameduser/Code/babl/outputs/t5-small/epoch=0-val_loss=10.70-val_EM=0.98-val_F1=0.90.ckpt\n",
      "/home/nameduser/.local/share/virtualenvs/babl-qoEDH0A2/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:476: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2a8d59772240889fdd0086c36ef080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_EM          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8000000715255737     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_F1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.984358012676239     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_EM         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8000000715255737    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_F1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.984358012676239    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<pytorch_lightning.trainer.trainer.Trainer at 0x7b1aaff75270>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from babl.models import MODELS_CHOICES, MODELS\n",
    "from babl.config import T5 as T5Config\n",
    "from pathlib import Path\n",
    "\n",
    "model_name = \"t5\"\n",
    "full_model_name = MODELS_CHOICES[model_name][0]\n",
    "t_w_m = MODELS[model_name]\n",
    "\n",
    "t = t_w_m[\"tok\"]\n",
    "m = t_w_m[\"model\"]\n",
    "\n",
    "tokenizer = t.from_pretrained(full_model_name)\n",
    "model = m.from_pretrained(full_model_name, **T5Config().__dict__)\n",
    "\n",
    "# placing in training mode \n",
    "\n",
    "model.train()\n",
    "\n",
    "data_path_root = Path(\"/home/nameduser/Code/babl/inputs\")\n",
    "\n",
    "# # data_path_val = data_path_root / \"10k.jsonl\"\n",
    "# # ds = TextDataset(data_path_val, tokenizer=t, plain_text=False)\n",
    "# # from babl.data import T2TDataCollator\n",
    "# # from torch.utils.data import DataLoader\n",
    "# # t_dl = DataLoader(ds, batch_size=64, shuffle=True, collate_fn=T2TDataCollator())\n",
    "# # test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "# # data_module = TextDataModule(data_path, tokenizer)\n",
    "\n",
    "Fitter(model=model, model_name=full_model_name, tokenizer=tokenizer, data_path=data_path_root)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for b in t_dl:\n",
    "#     # print(b)\n",
    "#     m(**b)\n",
    "\n",
    "import torch.nn.functional as F \n",
    "\n",
    "\n",
    "y = torch.tensor([[ 822,   10,  125, 100, 100, 100],\n",
    "                  [ 822,   10,  116, 100, 100, 100],\n",
    "                  [ 822,   10,  125, 100, 100, 100],\n",
    "                  [ 822,   10,  213, 100, 100, 100],\n",
    "                  [ 822,   10,  213, 100, 100, 100],\n",
    "                  [ 822,   10,  116, 100, 100, 100],\n",
    "                  [ 822,   10,  125, 100, 100, 100],\n",
    "                  [ 822,   10,  116, 100, 100, 100],\n",
    "                  [ 822,   10,  125, 100, 100, 100],\n",
    "                  [ 822,   10,  213, 100, 100, 100],\n",
    "                  [ 822,   10,  213, 100, 100, 100],\n",
    "                  [ 822,   10,  116, 100, 100, 100]], dtype=torch.long\n",
    ")\n",
    "y_hat = torch.tensor([[-20.2879,  -9.8936, -13.5965, -40.7275, -40.8642, -40.8486],\n",
    "         [-34.0870,  -3.6627, -14.2458,  -46.1296, -46.3147, -46.2990],\n",
    "         [-30.5974,  -3.4536, -15.5923,  -43.6581, -43.8461, -43.8219],\n",
    "         [-18.1922,  -8.0767, -14.5352,  -45.5706, -45.7357, -45.7194],\n",
    "         [-18.1516,  -8.0787, -14.4750,  -45.4796, -45.6429, -45.6272],\n",
    "         [-18.1262,  -8.1061, -14.4559,  -45.4136, -45.5755, -45.5602],\n",
    "         [-17.2200,  -9.7170, -14.2499,  -38.4455, -38.5326, -38.4609],\n",
    "         [-34.3804,  -6.2359, -13.2374,  -42.5014, -42.6473, -42.5558],\n",
    "         [-27.8060,  -7.1265, -15.4786,  -42.2502, -42.3610, -42.2977],\n",
    "         [-17.2795,  -7.8251, -15.8752,  -44.6078, -44.7242, -44.6339],\n",
    "         [-17.1784,  -7.7900, -15.8198,   -44.4029, -44.5184, -44.4275],\n",
    "         [-17.1213,  -7.7632, -15.7711,   -44.2831, -44.3977, -44.3082]]) \n",
    "\n",
    "num_class= 1321\n",
    "y_hat = torch.stack([y_hat]*num_class, dim=2)\n",
    "\n",
    "y_hat.shape\n",
    "y.shape\n",
    "# y.shape\n",
    "# y_hat.shape\n",
    "y = F.one_hot(y, num_classes=1321)\n",
    "y = y.float()\n",
    "\n",
    "y.shape == y_hat.shape\n",
    "\n",
    "y_hat = F.softmax(y_hat, dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "losses = []\n",
    "for tok in range(y_hat.shape[1]):\n",
    "    print(y[:,tok,:].shape)\n",
    "    print(y_hat[:,tok,:].shape)\n",
    "    loss = F.cross_entropy(y_hat[:,tok,:] , y[:,tok,:])\n",
    "    print(loss)\n",
    "    # loss = F.nll_loss(y_hat[:,tok,:] , y[:,tok,:])\n",
    "    losses.append(loss)\n",
    "\n",
    "\n",
    "torch.tensor(losses).mean()\n",
    "# yx = F.one_hot(y, num_classes=1321)[:,0,:].shape\n",
    "\n",
    "# loss = F.nll_loss(y_hat , y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path_root = Path(\"/home/nameduser/Code/babl/inputs\")\n",
    "\n",
    "data_path_val = data_path_root / \"10k.jsonl\"\n",
    "ds = TextDataset(data_path_val, tokenizer=t, plain_text=True )\n",
    "\n",
    "# from babl.data import T2TDataCollator\n",
    "# from torch.utils.data import DataLoader\n",
    "# t_dl = DataLoader(ds, batch_size=64, shuffle=True, collate_fn=T2TDataCollator())\n",
    "# test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "# data_module = TextDataModule(data_path, tokenizer)\n",
    "\n",
    "\n",
    "torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([0,1,1,0,0,1])\n",
    "\n",
    "yh=  torch.tensor([0,1,1,0,1,1])\n",
    "(y == yh).int()\n",
    "\n",
    "\n",
    "torch.prod(torch.tensor(torch.rand((10,10)).shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed (1414)\n",
    "\n",
    "t = torch.randn (8, 4)\n",
    "a = t.argmax(1)\n",
    "m = torch.zeros(t.shape).scatter(1, a.unsqueeze(1), 1.0)\n",
    "\n",
    "\n",
    "print ('\\n', t, '\\n\\n', a, '\\n\\n', m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list({\"x\": [1, 2, 3, 4]}.values())[0].__len__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babl-qoEDH0A2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
