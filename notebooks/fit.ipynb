{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to pytorch lightning fitting routine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorboard\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 32128\n",
    "\n",
    "class Routine(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = 1e-3\n",
    "        self.validation_step_outputs = []\n",
    "        self.training_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        labels,\n",
    "        decoder_attention_mask, \n",
    "    ):\n",
    "        y_hat = self.model(input_ids=input_ids, attention_mask=attention_mask, decoder_attention_mask=decoder_attention_mask, labels=labels)\n",
    "        \n",
    "        # print(f\"forward(): {y_hat=}\")\n",
    "        return y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # print(f\"keys = {batch.keys()}\")\n",
    "        # print(f\"{batch=}\")\n",
    "        y = batch['labels']\n",
    "        y_hat = self(**batch)\n",
    "        y_onehot = F.one_hot(y, num_classes=VOCAB_SIZE)\n",
    "        y = y_onehot.float()\n",
    "        losses = []\n",
    "        # computing cross-entropy on per-token basis and averaging the loss. \n",
    "        for tok in range(y_hat.logits.shape[1]):\n",
    "            # print(\"Per-token loss cross entropy\")\n",
    "            loss = F.cross_entropy(y_hat.logits[:,tok,:] , y[:,tok,:])\n",
    "            # print(loss)\n",
    "            # loss = F.nll_loss(y_hat[:,tok,:] , y[:,tok,:])\n",
    "            losses.append(loss)\n",
    "\n",
    "        loss  = torch.tensor(losses,  requires_grad=True).mean()\n",
    "        # dummy metrics\n",
    "\n",
    "\n",
    "        # calculating exact matches \n",
    "        y_hat = F.softmax(y_hat.logits, dim=-1)\n",
    "        a = y_hat.argmax(1)\n",
    "        y_hat = torch.zeros(y_hat.shape).scatter(1, a.unsqueeze(1), 1.0)\n",
    "        # y_onehot = F.one_hot(y, num_classes=VOCAB_SIZE)\n",
    "        # y = y_onehot.float()\n",
    "        matches = (y == y_hat).int()\n",
    "        correct = matches.sum()\n",
    "        tot = torch.prod(torch.tensor(matches.shape))\n",
    "        metrics_dict = {\"loss\": loss, \"train_EM\": (correct/tot).item(), \"train_F1\": 0.9}\n",
    "        # print(metrics_dict)\n",
    "        self.training_step_outputs.append(metrics_dict)\n",
    "        return metrics_dict\n",
    "\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        results = {\n",
    "            \"loss\": torch.tensor(\n",
    "                [x[\"loss\"] for x in self.training_step_outputs]\n",
    "            ).mean(),\n",
    "            \"F1\": torch.tensor(\n",
    "                [x[\"train_F1\"] for x in self.training_step_outputs]\n",
    "            ).mean(),\n",
    "            \"EM\": torch.tensor(\n",
    "                [x[\"train_EM\"] for x in self.training_step_outputs]\n",
    "            ).mean(),\n",
    "        }\n",
    "        # self.log(f\"LR\",self.lr, on_epoch=True, prog_bar=True, logger=True)\n",
    "        for k, v in results.items():\n",
    "            self.log(\n",
    "                f\"train_{k}\",\n",
    "                v,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "                sync_dist=True,\n",
    "            )\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y = batch['labels']\n",
    "        y_hat = self(**batch)\n",
    "        y_onehot = F.one_hot(y, num_classes=VOCAB_SIZE)\n",
    "        y = y_onehot.float()\n",
    "        losses = []\n",
    "        # computing cross-entropy on per-token basis and averaging the loss. \n",
    "        for tok in range(y_hat.logits.shape[1]):\n",
    "            loss = F.cross_entropy(y_hat.logits[:,tok,:] , y[:,tok,:])\n",
    "            # print(loss)\n",
    "            losses.append(loss)\n",
    "        loss  = torch.tensor(losses).mean()\n",
    "\n",
    "\n",
    "\n",
    "        # calculating exact matches \n",
    "        y_hat = F.softmax(y_hat.logits, dim=-1)\n",
    "        a = y_hat.argmax(1)\n",
    "        y_hat = torch.zeros(y_hat.shape).scatter(1, a.unsqueeze(1), 1.0)\n",
    "        matches = (y == y_hat).int()\n",
    "        correct = matches.sum()\n",
    "        tot = torch.prod(torch.tensor(matches.shape))\n",
    "        \n",
    "\n",
    "\n",
    "        # dummy metrics\n",
    "        metrics_dict = {\"val_loss\": loss.item(), \"val_EM\": (correct/tot).item(), \"val_F1\": 0.9}\n",
    "        self.validation_step_outputs.append(metrics_dict)\n",
    "        return metrics_dict\n",
    "\n",
    "\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        results = {\n",
    "            \"loss\": torch.tensor(\n",
    "                [x[\"val_loss\"] for x in self.validation_step_outputs]\n",
    "            ).mean(),\n",
    "            \"EM\": torch.tensor(\n",
    "                [x[\"val_EM\"] for x in self.validation_step_outputs]\n",
    "            ).mean(),\n",
    "            \"F1\": torch.tensor(\n",
    "                [x[\"val_F1\"] for x in self.validation_step_outputs]\n",
    "            ).mean(),\n",
    "        }\n",
    "        for k, v in results.items():\n",
    "            self.log(\n",
    "                f\"val_{k}\", v, on_epoch=True, prog_bar=True, logger=True, sync_dist=True\n",
    "            )\n",
    "            # self.log(f\"val_{k}\", v, on_epoch=True, prog_bar=True) # , logger=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # x = batch[\"x\"]\n",
    "        \n",
    "        \n",
    "        y = batch[\"labels\"]\n",
    "        y_hat = self(**batch)\n",
    "        \n",
    "        # calculating exact matches \n",
    "        y_hat = F.softmax(y_hat.logits, dim=-1)\n",
    "        a = y_hat.argmax(1)\n",
    "        y_hat = torch.zeros(y_hat.shape).scatter(1, a.unsqueeze(1), 1.0)\n",
    "        y_onehot = F.one_hot(y, num_classes=VOCAB_SIZE)\n",
    "        y = y_onehot.float()\n",
    "        matches = (y == y_hat).int()\n",
    "        correct = matches.sum()\n",
    "        tot = torch.prod(torch.tensor(matches.shape))\n",
    "        \n",
    "        \n",
    "        metrics_dict = {\n",
    "            \"test_EM\": (correct/tot).item(),\n",
    "            \"test_F1\": 0.8,\n",
    "        }\n",
    "        self.test_step_outputs.append(metrics_dict)\n",
    "        return metrics_dict\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        results = {\n",
    "            \"F1\": torch.tensor([x[\"test_EM\"] for x in self.test_step_outputs]).mean(),\n",
    "            \"EM\": torch.tensor([x[\"test_F1\"] for x in self.test_step_outputs]).mean(),\n",
    "        }\n",
    "\n",
    "        for k, v in results.items():\n",
    "            self.log(\n",
    "                f\"test_{k}\",\n",
    "                v,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "                sync_dist=True,\n",
    "            )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        # special scheduler for transformers\n",
    "        optimizer = optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=0.001,  # self.cfg_fitting.learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-08,\n",
    "            weight_decay=0.05,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"monitor\": \"val_loss\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from babl.data import TextDataset, TextDataModule\n",
    "from babl.utils import CallbackCollection\n",
    "\n",
    "class Fitter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        model_name,\n",
    "        data_path=\"../inputs\",\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_name = model_name\n",
    "        self.data_path = data_path\n",
    "\n",
    "        ####################################################################################\n",
    "        @dataclass\n",
    "        class FittingArgs:\n",
    "            es_patience: int = 5\n",
    "            model_dir = Path(\"/home/nameduser/Code/babl/outputs\") / model_name\n",
    "            max_epoch: int = 10\n",
    "            fast_dev_run: bool = False\n",
    "\n",
    "            def __post_init__(self):\n",
    "                self.model_dir.mkdir(parents=True, exist_ok=True)\n",
    "                self.model_dir  =  str(self.model_dir)\n",
    "\n",
    "        ####################################################################################\n",
    "\n",
    "        self.args = FittingArgs()\n",
    "\n",
    "    def setup(self):\n",
    "        data_module = TextDataModule(data_path=self.data_path, tokenizer=self.tokenizer, dev_run=True)\n",
    "\n",
    "        train_loader = data_module.train_dataloader()\n",
    "        val_loader = data_module.val_dataloader()\n",
    "        test_loader = data_module.test_dataloader()\n",
    "\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    def callbacks(self):\n",
    "        # cfg_fitting = self.cfg_fitting\n",
    "        callback_collection = CallbackCollection(self.data_path, self.args)\n",
    "        return callback_collection()\n",
    "\n",
    "    def __call__(self):\n",
    "\n",
    "        logger = TensorBoardLogger(\n",
    "            save_dir=self.args.model_dir,\n",
    "            name=\"lightning_logs\",\n",
    "        )\n",
    "        Model = self.model\n",
    "        # get loaders and datamodule to access input shape\n",
    "        train_loader, val_loader, test_loader = self.setup()\n",
    "        print(\"Created training, validating and test loaders .... \")\n",
    "        # get input shape for onnx exporting\n",
    "        # input_shape = data_module.input_shape\n",
    "        # init model\n",
    "        # kwargs = {}\n",
    "        # model = Model(**kwargs)\n",
    "\n",
    "        # setup training, validating and testing routines for the model\n",
    "        routine = Routine(self.model)\n",
    "\n",
    "        # Init a trainer to execute routine\n",
    "        callback_dict = self.callbacks()\n",
    "        callback_list = [v for (_, v) in callback_dict.items()]\n",
    "        number_devices = os.getenv(\"CUDA_VISIBLE_DEVICES\", \"1,\").split(\",\")\n",
    "        try:\n",
    "            number_devices.remove(\"\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        trainer = Trainer(\n",
    "            accelerator=\"cpu\",\n",
    "            devices=len(number_devices),\n",
    "            # strategy=os.getenv(\"STRATEGY\", \"ddp_notebook\"),\n",
    "            sync_batchnorm=True,\n",
    "            logger=logger,\n",
    "            max_epochs=self.args.max_epoch,\n",
    "            callbacks=callback_list,\n",
    "            num_sanity_val_steps=2,\n",
    "            # resume_from_checkpoint=self.cfg_fitting.resume_from_checkpoint,\n",
    "            gradient_clip_val=1.0,\n",
    "            fast_dev_run=self.args.fast_dev_run,\n",
    "        )\n",
    "\n",
    "        trainer.fit(\n",
    "            routine, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    "        )  # ,ckpt_path=PATH)\n",
    "\n",
    "        if self.args.fast_dev_run:\n",
    "            # issue with finding best weights path for in fast dev run using last model weights\n",
    "            model_ckpt_path = callback_dict[\"checkpoint\"].__dict__[\"last_model_path\"]\n",
    "        else:\n",
    "            model_ckpt_path = callback_dict[\"checkpoint\"].__dict__[\"best_model_path\"]\n",
    "\n",
    "        trainer.test(\n",
    "            dataloaders=test_loader,\n",
    "            ckpt_path=model_ckpt_path,\n",
    "        )\n",
    "        # Return the input_shapes and trainer of the model for exporting\n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babl.models import MODELS_CHOICES, MODELS\n",
    "from babl.config import T5 as T5Config\n",
    "from pathlib import Path\n",
    "\n",
    "model_name = \"t5\"\n",
    "full_model_name = MODELS_CHOICES[model_name][0]\n",
    "t_w_m = MODELS[model_name]\n",
    "\n",
    "t = t_w_m[\"tok\"]\n",
    "m = t_w_m[\"model\"]\n",
    "\n",
    "tokenizer = t.from_pretrained(full_model_name)\n",
    "model = m.from_pretrained(full_model_name, **T5Config().__dict__)\n",
    "\n",
    "# placing in training mode \n",
    "\n",
    "model.train()\n",
    "\n",
    "data_path_root = Path(\"/home/nameduser/Code/babl/inputs\")\n",
    "\n",
    "# # data_path_val = data_path_root / \"10k.jsonl\"\n",
    "# # ds = TextDataset(data_path_val, tokenizer=t, plain_text=False)\n",
    "# # from babl.data import T2TDataCollator\n",
    "# # from torch.utils.data import DataLoader\n",
    "# # t_dl = DataLoader(ds, batch_size=64, shuffle=True, collate_fn=T2TDataCollator())\n",
    "# # test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "# # data_module = TextDataModule(data_path, tokenizer)\n",
    "\n",
    "Fitter(model=model, model_name=full_model_name, tokenizer=tokenizer, data_path=data_path_root)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for b in t_dl:\n",
    "#     # print(b)\n",
    "#     m(**b)\n",
    "\n",
    "import torch.nn.functional as F \n",
    "\n",
    "\n",
    "y = torch.tensor([[ 822,   10,  125, 100, 100, 100],\n",
    "                  [ 822,   10,  116, 100, 100, 100],\n",
    "                  [ 822,   10,  125, 100, 100, 100],\n",
    "                  [ 822,   10,  213, 100, 100, 100],\n",
    "                  [ 822,   10,  213, 100, 100, 100],\n",
    "                  [ 822,   10,  116, 100, 100, 100],\n",
    "                  [ 822,   10,  125, 100, 100, 100],\n",
    "                  [ 822,   10,  116, 100, 100, 100],\n",
    "                  [ 822,   10,  125, 100, 100, 100],\n",
    "                  [ 822,   10,  213, 100, 100, 100],\n",
    "                  [ 822,   10,  213, 100, 100, 100],\n",
    "                  [ 822,   10,  116, 100, 100, 100]], dtype=torch.long\n",
    ")\n",
    "y_hat = torch.tensor([[-20.2879,  -9.8936, -13.5965, -40.7275, -40.8642, -40.8486],\n",
    "         [-34.0870,  -3.6627, -14.2458,  -46.1296, -46.3147, -46.2990],\n",
    "         [-30.5974,  -3.4536, -15.5923,  -43.6581, -43.8461, -43.8219],\n",
    "         [-18.1922,  -8.0767, -14.5352,  -45.5706, -45.7357, -45.7194],\n",
    "         [-18.1516,  -8.0787, -14.4750,  -45.4796, -45.6429, -45.6272],\n",
    "         [-18.1262,  -8.1061, -14.4559,  -45.4136, -45.5755, -45.5602],\n",
    "         [-17.2200,  -9.7170, -14.2499,  -38.4455, -38.5326, -38.4609],\n",
    "         [-34.3804,  -6.2359, -13.2374,  -42.5014, -42.6473, -42.5558],\n",
    "         [-27.8060,  -7.1265, -15.4786,  -42.2502, -42.3610, -42.2977],\n",
    "         [-17.2795,  -7.8251, -15.8752,  -44.6078, -44.7242, -44.6339],\n",
    "         [-17.1784,  -7.7900, -15.8198,   -44.4029, -44.5184, -44.4275],\n",
    "         [-17.1213,  -7.7632, -15.7711,   -44.2831, -44.3977, -44.3082]]) \n",
    "\n",
    "num_class= 1321\n",
    "y_hat = torch.stack([y_hat]*num_class, dim=2)\n",
    "\n",
    "y_hat.shape\n",
    "y.shape\n",
    "# y.shape\n",
    "# y_hat.shape\n",
    "y = F.one_hot(y, num_classes=1321)\n",
    "y = y.float()\n",
    "\n",
    "y.shape == y_hat.shape\n",
    "\n",
    "y_hat = F.softmax(y_hat, dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "losses = []\n",
    "for tok in range(y_hat.shape[1]):\n",
    "    print(y[:,tok,:].shape)\n",
    "    print(y_hat[:,tok,:].shape)\n",
    "    loss = F.cross_entropy(y_hat[:,tok,:] , y[:,tok,:])\n",
    "    print(loss)\n",
    "    # loss = F.nll_loss(y_hat[:,tok,:] , y[:,tok,:])\n",
    "    losses.append(loss)\n",
    "\n",
    "\n",
    "torch.tensor(losses).mean()\n",
    "# yx = F.one_hot(y, num_classes=1321)[:,0,:].shape\n",
    "\n",
    "# loss = F.nll_loss(y_hat , y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path_root = Path(\"/home/nameduser/Code/babl/inputs\")\n",
    "\n",
    "data_path_val = data_path_root / \"10k.jsonl\"\n",
    "ds = TextDataset(data_path_val, tokenizer=t, plain_text=True )\n",
    "\n",
    "# from babl.data import T2TDataCollator\n",
    "# from torch.utils.data import DataLoader\n",
    "# t_dl = DataLoader(ds, batch_size=64, shuffle=True, collate_fn=T2TDataCollator())\n",
    "# test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "# data_module = TextDataModule(data_path, tokenizer)\n",
    "\n",
    "\n",
    "torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([0,1,1,0,0,1])\n",
    "\n",
    "yh=  torch.tensor([0,1,1,0,1,1])\n",
    "(y == yh).int()\n",
    "\n",
    "\n",
    "torch.prod(torch.tensor(torch.rand((10,10)).shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed (1414)\n",
    "\n",
    "t = torch.randn (8, 4)\n",
    "a = t.argmax(1)\n",
    "m = torch.zeros(t.shape).scatter(1, a.unsqueeze(1), 1.0)\n",
    "\n",
    "\n",
    "print ('\\n', t, '\\n\\n', a, '\\n\\n', m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list({\"x\": [1, 2, 3, 4]}.values())[0].__len__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babl-qoEDH0A2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
